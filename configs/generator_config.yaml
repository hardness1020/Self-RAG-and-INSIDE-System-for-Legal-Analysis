# Generator Model Configuration

# Base model
model:
  base_model: "meta-llama/Llama-2-7b-hf"  # Options: Llama-2-7b-hf, mistralai/Mistral-7B-v0.1
  model_type: "llama"
  cache_dir: "models/base"

# Quantization (for memory efficiency)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA configuration
lora:
  r: 16                        # LoRA rank
  lora_alpha: 32               # LoRA scaling factor
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training data
data:
  training_data_dir: "data/training/generator_data"
  num_samples: 5000            # Number of augmented training samples
  validation_split: 0.1
  max_seq_length: 1024         # Longer context for generation

# Reflection token vocabulary
reflection_tokens:
  intent:  # NEW: INSIDE intent tokens
    - "[Intent:Factual]"
    - "[Intent:Exploratory]"
    - "[Intent:Comparative]"
    - "[Intent:Procedural]"
    - "[Intent:Unknown]"
  retrieve:
    - "[Retrieve]"
  relevance:
    - "[Relevant]"
    - "[Irrelevant]"
  support:
    - "[Fully Supported]"
    - "[Partially Supported]"
    - "[No Support]"
  utility:
    - "[Utility:5]"
    - "[Utility:4]"
    - "[Utility:3]"
    - "[Utility:2]"
    - "[Utility:1]"

# Training hyperparameters
training:
  output_dir: "models/generator_lora"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 10
  save_steps: 200
  eval_steps: 200
  save_total_limit: 3
  fp16: false
  bf16: false
  optim: "paged_adamw_32bit"
  gradient_checkpointing: true
  max_grad_norm: 0.3

# Inference parameters
inference:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  num_beams: 1

  # Reflection token weights for scoring
  weights:
    w_isrel: 1.0     # Weight for relevance token
    w_issup: 1.0     # Weight for support token
    w_isuse: 1.0     # Weight for utility token

  # Adaptive retrieval
  adaptive_retrieval: true
  retrieval_threshold: 0.5  # Confidence threshold for triggering retrieval

# Retrieval integration
retrieval:
  config_path: "configs/retrieval_config.yaml"
  num_passages: 3   # Number of passages to retrieve per query

# INSIDE Hallucination Detection Integration (NEW)
inside:
  enabled: true
  config_path: "configs/inside_config.yaml"

  # Internal states extraction during generation
  extract_internal_states: true
  target_layers: [16]  # Middle layer for Llama-2-7B (32 layers total)

  # EigenScore hallucination detection
  eigenscore:
    enabled: true
    compute_per_generation: true  # Compute EigenScore for each generation
    threshold: 5.0                # Hallucination threshold (lower = hallucination)
    use_adaptive_threshold: true  # Adjust based on query characteristics

  # Feature clipping (optional, expensive)
  feature_clipping:
    enabled: false                # Disabled by default (high computational cost)
    percentile: 95.0              # Clipping percentile

  # Combined scoring (Self-RAG + INSIDE)
  combined_scoring:
    use_eigenscore: true
    eigenscore_weight: 0.3        # Weight for EigenScore
    reflection_weight: 0.7        # Weight for reflection tokens (ISSUP, ISUSE)
