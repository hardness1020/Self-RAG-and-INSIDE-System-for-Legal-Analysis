# Critic Model Configuration

# Base model
model:
  base_model: "meta-llama/Llama-2-7b-hf"  # Options: Llama-2-7b-hf, mistralai/Mistral-7B-v0.1
  model_type: "llama"
  cache_dir: "models/base"

# Quantization (for memory efficiency)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA configuration
lora:
  r: 16                        # LoRA rank
  lora_alpha: 32               # LoRA scaling factor
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training data
data:
  training_data_dir: "data/training/critic_data"
  num_samples_per_token: 2000  # Number of samples per reflection token type
  validation_split: 0.1
  max_seq_length: 512

# Reflection tokens to train
reflection_tokens:
  - "INTENT"        # [INTENT] intent token (NEW - INSIDE extension)
  - "Retrieve"      # [Retrieve] token
  - "ISREL"         # [ISREL] relevance token
  - "ISSUP"         # [ISSUP] support token
  - "ISUSE"         # [ISUSE] utility token

# Training hyperparameters
training:
  output_dir: "models/critic_lora"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 3
  fp16: false
  bf16: false
  optim: "paged_adamw_32bit"
  gradient_checkpointing: true
  max_grad_norm: 0.3

# Label generation (if using GPT-4)
label_generation:
  use_gpt4: false              # Set to true if you have OpenAI API access
  openai_model: "gpt-4"
  temperature: 0.0
  max_tokens: 50
  few_shot_examples: 5
