{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Self-RAG\n",
    "\n",
    "This notebook helps you prepare your legal corpus and Q&A data for training the Self-RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.175322Z",
     "iopub.status.busy": "2025-10-31T07:33:56.175156Z",
     "iopub.status.idle": "2025-10-31T07:33:56.393282Z",
     "shell.execute_reply": "2025-10-31T07:33:56.392891Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.retrieval.chunking import DocumentChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Documents\n",
    "\n",
    "Start with the provided sample data or load your own legal documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.394734Z",
     "iopub.status.busy": "2025-10-31T07:33:56.394633Z",
     "iopub.status.idle": "2025-10-31T07:33:56.396646Z",
     "shell.execute_reply": "2025-10-31T07:33:56.396381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n",
      "\n",
      "First document:\n",
      "{\n",
      "  \"text\": \"To establish negligence, a plaintiff must prove four essential elements: (1) duty of care, (2) breach of that duty, (3) causation, and (4) damages. Each element must be proven by a preponderance of the evidence, meaning it is more likely than not that the defendant was negligent. The duty of care arises from the relationship between the parties and the foreseeability of harm.\",\n",
      "  \"source\": \"negligence_basics.txt\",\n",
      "  \"title\": \"Elements of Negligence\",\n",
      "  \"doc_id\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents\n",
    "with open('../data/samples/sample_documents.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst document:\")\n",
    "print(json.dumps(documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Document Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.409659Z",
     "iopub.status.busy": "2025-10-31T07:33:56.409575Z",
     "iopub.status.idle": "2025-10-31T07:33:56.421526Z",
     "shell.execute_reply": "2025-10-31T07:33:56.421149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Statistics:\n",
      "       text_length  word_count\n",
      "count    10.000000   10.000000\n",
      "mean    352.000000   54.300000\n",
      "std      23.911875    6.481598\n",
      "min     319.000000   47.000000\n",
      "25%     332.500000   49.250000\n",
      "50%     357.000000   53.000000\n",
      "75%     367.250000   60.500000\n",
      "max     389.000000   63.000000\n",
      "\n",
      "Sources: ['negligence_basics.txt' 'causation.txt' 'damages.txt' 'duty_of_care.txt'\n",
      " 'res_ipsa.txt' 'negligence_per_se.txt' 'defenses.txt'\n",
      " 'professional_negligence.txt']\n",
      "Average words per document: 54\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "# Add text length\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Document Statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())\n",
    "\n",
    "print(f\"\\nSources: {df['source'].unique()}\")\n",
    "print(f\"Average words per document: {df['word_count'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Document Chunking\n",
    "\n",
    "Test different chunking strategies to find optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.422449Z",
     "iopub.status.busy": "2025-10-31T07:33:56.422383Z",
     "iopub.status.idle": "2025-10-31T07:33:56.424596Z",
     "shell.execute_reply": "2025-10-31T07:33:56.424220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 21\n",
      "Average chunks per document: 2.1\n",
      "\n",
      "Example chunks from first document:\n",
      "\n",
      "Chunk 1:\n",
      "To establish negligence, a plaintiff must prove four essential elements: (1) duty of care, (2) breach of that duty, (3) causation, and (4) damages. ...\n",
      "\n",
      "Chunk 2:\n",
      "Each element must be proven by a preponderance of the evidence, meaning it is more likely than not that the defendant was negligent. . The duty of car...\n"
     ]
    }
   ],
   "source": [
    "# Create chunker\n",
    "chunker = DocumentChunker({\n",
    "    'chunk_size': 256,\n",
    "    'chunk_overlap': 30\n",
    "})\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = chunker.chunk_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Average chunks per document: {len(all_chunks) / len(documents):.1f}\")\n",
    "\n",
    "# Show example chunks\n",
    "print(\"\\nExample chunks from first document:\")\n",
    "doc_0_chunks = [c for c in all_chunks if c['doc_id'] == 0]\n",
    "for i, chunk in enumerate(doc_0_chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk['text'][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Q&A Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.425616Z",
     "iopub.status.busy": "2025-10-31T07:33:56.425489Z",
     "iopub.status.idle": "2025-10-31T07:33:56.428284Z",
     "shell.execute_reply": "2025-10-31T07:33:56.428000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 Q&A pairs\n",
      "\n",
      "Sample Q&A:\n",
      "\n",
      "Q: What are the four elements that must be proven to establish negligence?\n",
      "A: To establish negligence, a plaintiff must prove: (1) duty of care - the defendant owed a legal duty ...\n",
      "\n",
      "Q: What is the standard for determining breach of duty in negligence cases?\n",
      "A: The standard for breach of duty is objective and based on what a reasonable person would do in simil...\n"
     ]
    }
   ],
   "source": [
    "# Load Q&A data\n",
    "with open('../data/samples/sample_qa_data.json', 'r') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(qa_data)} Q&A pairs\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "qa_df = pd.DataFrame(qa_data)\n",
    "\n",
    "print(\"\\nSample Q&A:\")\n",
    "for i, row in qa_df.head(2).iterrows():\n",
    "    print(f\"\\nQ: {row['question']}\")\n",
    "    print(f\"A: {row['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:56.429223Z",
     "iopub.status.busy": "2025-10-31T07:33:56.429167Z",
     "iopub.status.idle": "2025-10-31T07:33:57.020034Z",
     "shell.execute_reply": "2025-10-31T07:33:57.019610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8 examples\n",
      "Test set: 2 examples\n",
      "\n",
      "Saved train and test splits!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80/20\n",
    "train_qa, test_qa = train_test_split(qa_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_qa)} examples\")\n",
    "print(f\"Test set: {len(test_qa)} examples\")\n",
    "\n",
    "# Save splits\n",
    "Path('../data/training').mkdir(parents=True, exist_ok=True)\n",
    "with open('../data/training/train_qa.json', 'w') as f:\n",
    "    json.dump(train_qa, f, indent=2)\n",
    "\n",
    "with open('../data/training/test_qa.json', 'w') as f:\n",
    "    json.dump(test_qa, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved train and test splits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Your Own Data\n",
    "\n",
    "Use this template to load your own legal documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T07:33:57.021109Z",
     "iopub.status.busy": "2025-10-31T07:33:57.021004Z",
     "iopub.status.idle": "2025-10-31T07:33:57.023039Z",
     "shell.execute_reply": "2025-10-31T07:33:57.022718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Template for loading your own documents\n",
    "def load_custom_documents(directory_path):\n",
    "    \"\"\"\n",
    "    Load documents from a directory.\n",
    "    Modify this function based on your data format.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Example: Load from text files\n",
    "    for file_path in Path(directory_path).glob('*.txt'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        documents.append({\n",
    "            'text': text,\n",
    "            'source': file_path.name,\n",
    "            'title': file_path.stem,\n",
    "            'doc_id': len(documents)\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Uncomment to use:\n",
    "# my_documents = load_custom_documents('../data/my_legal_corpus')\n",
    "# print(f\"Loaded {len(my_documents)} custom documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preparation complete! You now have:\n",
    "- ✅ Loaded and explored documents\n",
    "- ✅ Tested chunking strategies\n",
    "- ✅ Prepared Q&A data\n",
    "- ✅ Created train/test splits\n",
    "\n",
    "**Next Steps:**\n",
    "1. Go to `02_retrieval_pipeline.ipynb` to build the retrieval system\n",
    "2. Or proceed to `03_self_rag_training.ipynb` to train models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag-legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
