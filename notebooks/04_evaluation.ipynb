{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Self-RAG Performance\n",
    "\n",
    "Comprehensive evaluation of retrieval and generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluate Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run retrieval evaluation\n",
    "uv run python -m src.evaluation.retrieval_eval \\\n",
    "    --config ../configs/retrieval_config.yaml \\\n",
    "    --index-dir ../data/embeddings \\\n",
    "    --test-data ../data/samples/sample_test_queries.json \\\n",
    "    --output ../results/retrieval_results.json\n",
    "\n",
    "echo \"✅ Retrieval evaluation complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('../results/retrieval_results.json', 'r') as f:\n",
    "    ret_results = json.load(f)\n",
    "\n",
    "print(\"Retrieval Metrics:\")\n",
    "print(json.dumps(ret_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Precision@k and Recall@k\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Precision@k\n",
    "prec = ret_results['precision@k']\n",
    "ax1.bar(prec.keys(), prec.values(), color='skyblue')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Precision@k')\n",
    "ax1.set_title('Retrieval Precision')\n",
    "\n",
    "# Recall@k\n",
    "rec = ret_results['recall@k']\n",
    "ax2.bar(rec.keys(), rec.values(), color='lightgreen')\n",
    "ax2.set_xlabel('k')\n",
    "ax2.set_ylabel('Recall@k')\n",
    "ax2.set_title('Retrieval Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate Generation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run generation evaluation\n",
    "uv run python -m src.evaluation.generation_eval \\\n",
    "    --retrieval-config ../configs/retrieval_config.yaml \\\n",
    "    --generator-config ../configs/generator_config.yaml \\\n",
    "    --index-dir ../data/embeddings \\\n",
    "    --generator-weights ../models/generator_lora/final \\\n",
    "    --test-data ../data/samples/sample_qa_data.json \\\n",
    "    --output ../results/generation_results.json\n",
    "\n",
    "echo \"✅ Generation evaluation complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('../results/generation_results.json', 'r') as f:\n",
    "    gen_results = json.load(f)\n",
    "\n",
    "print(\"Generation Metrics:\")\n",
    "print(json.dumps(gen_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generation metrics\n",
    "metrics = {\n",
    "    'Hallucination Rate': gen_results['hallucination_rate'],\n",
    "    'FactScore': gen_results['avg_factscore'],\n",
    "    'Utility Score': gen_results['avg_utility_score'],\n",
    "    'Completeness': gen_results['avg_completeness']\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['red' if 'Hallucination' in k else 'green' for k in metrics.keys()]\n",
    "ax.barh(list(metrics.keys()), list(metrics.values()), color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_title('Self-RAG Generation Quality')\n",
    "ax.set_xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare with Baselines (Optional)\n",
    "\n",
    "Compare Self-RAG with vanilla RAG or no-RAG baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparison data (replace with actual results)\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['No RAG', 'Vanilla RAG', 'Self-RAG'],\n",
    "    'Hallucination Rate': [0.45, 0.30, 0.20],\n",
    "    'FactScore': [0.55, 0.65, 0.75],\n",
    "    'Utility': [0.60, 0.70, 0.80]\n",
    "})\n",
    "\n",
    "comparison.set_index('Model').plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Metric')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Reflection Tokens\n",
    "\n",
    "Analyze how reflection tokens correlate with quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example analysis (replace with actual data)\n",
    "token_analysis = pd.DataFrame({\n",
    "    'Support Level': ['Fully Supported', 'Partially Supported', 'No Support'],\n",
    "    'Count': [7, 2, 1],\n",
    "    'Avg Quality': [0.85, 0.65, 0.40]\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.bar(token_analysis['Support Level'], token_analysis['Count'], color='steelblue')\n",
    "ax1.set_title('Distribution of Support Levels')\n",
    "ax1.set_ylabel('Count')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "ax2.bar(token_analysis['Support Level'], token_analysis['Avg Quality'], color='coral')\n",
    "ax2.set_title('Quality by Support Level')\n",
    "ax2.set_ylabel('Average Quality Score')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Evaluation complete!\n",
    "- ✅ Retrieval metrics computed\n",
    "- ✅ Generation quality measured\n",
    "- ✅ Hallucination rate assessed\n",
    "- ✅ Visualizations created\n",
    "\n",
    "**Next:** Use `05_demo.ipynb` for interactive demonstration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
