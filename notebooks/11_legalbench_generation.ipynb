{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: LegalBench Generation Evaluation\n",
    "\n",
    "**Objective:** Compare generation methods on LegalBench-RAG benchmark\n",
    "\n",
    "**Methods:**\n",
    "1. **No-RAG**: Direct generation without retrieval (baseline)\n",
    "2. **Basic RAG**: Retrieve once, then generate\n",
    "3. **Self-RAG**: Adaptive retrieval with reflection tokens\n",
    "4. **Self-RAG + INSIDE**: Self-RAG enhanced with hallucination detection\n",
    "\n",
    "**Metrics:**\n",
    "- **F1 Score**: Token-level overlap with ground truth\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "- **Hallucination Rate**: From reflection tokens (ISSUP)\n",
    "- **Utility Score**: From ISUSE tokens\n",
    "- **EigenScore**: Internal state-based hallucination detection (INSIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LegalBench mini dataset\n",
    "QUERIES_FILE = \"../data/legalbench-rag/queries.json\"\n",
    "NUM_QUERIES = 776  # Mini dataset\n",
    "\n",
    "with open(QUERIES_FILE, 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "queries = queries_data['tests'][:NUM_QUERIES]\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "print(f\"\\nExample query:\")\n",
    "print(f\"  Query: {queries[0]['query'][:100]}...\")\n",
    "print(f\"  Dataset: {queries[0]['dataset_source']}\")\n",
    "print(f\"  Num snippets: {len(queries[0]['snippets'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count queries by subdataset\n",
    "dataset_counts = defaultdict(int)\n",
    "for q in queries:\n",
    "    dataset_counts[q['dataset_source']] += 1\n",
    "\n",
    "print(\"Queries by subdataset:\")\n",
    "for dataset, count in sorted(dataset_counts.items()):\n",
    "    print(f\"  {dataset}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.retriever import LegalRetriever\n",
    "from retrieval.embedding import EmbeddingModel\n",
    "\n",
    "# Load retriever\n",
    "print(\"Loading retriever...\")\n",
    "embedding_model = EmbeddingModel(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "retriever = LegalRetriever(embedding_model=embedding_model, top_k=3)\n",
    "retriever.load_index(\"../data/legalbench_embeddings\")\n",
    "\n",
    "print(\"✓ Retriever loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Generator Model\n",
    "\n",
    "**Note:** This notebook expects trained Self-RAG models. If training is not complete:\n",
    "- Base model will be used (no LoRA)\n",
    "- Reflection tokens may not be accurate\n",
    "- Results will demonstrate the pipeline, not final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_rag.generator import SelfRAGGenerator\n",
    "from self_rag.critic import Critic\n",
    "from self_rag.inference import SelfRAGPipeline\n",
    "\n",
    "# Load generator config\n",
    "with open('../configs/generator_config.yaml', 'r') as f:\n",
    "    generator_config = yaml.safe_load(f)\n",
    "\n",
    "# Load critic config\n",
    "with open('../configs/critic_config.yaml', 'r') as f:\n",
    "    critic_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loading generator model...\")\n",
    "generator = SelfRAGGenerator(generator_config)\n",
    "\n",
    "print(\"Loading critic model...\")\n",
    "critic = Critic(critic_config)\n",
    "\n",
    "print(\"✓ Models loaded\")\n",
    "\n",
    "# Check if LoRA weights exist\n",
    "lora_path = Path(generator_config['model']['lora_weights_path'])\n",
    "if lora_path.exists():\n",
    "    print(\"✓ Using trained LoRA weights\")\n",
    "else:\n",
    "    print(\"⚠️  LoRA weights not found - using base model\")\n",
    "    print(\"   Train models first with: python src/training/train_generator_qlora.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement 4 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: No-RAG (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_rag_generation(query: str, generator: SelfRAGGenerator) -> dict:\n",
    "    \"\"\"\n",
    "    Generate answer without retrieval (baseline).\n",
    "    \n",
    "    Args:\n",
    "        query: Legal question\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and metadata\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Answer the legal question based on your knowledge. Be concise and factual.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    result = generator.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,  # Greedy for reproducibility\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': result['generated_text'],\n",
    "        'method': 'No-RAG',\n",
    "        'num_tokens': len(result['generated_text'].split()),\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = no_rag_generation(queries[0]['query'], generator)\n",
    "print(\"No-RAG test:\")\n",
    "print(f\"  Answer: {test_result['answer'][:100]}...\")\n",
    "print(f\"  Length: {test_result['num_tokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rag_generation(query: str, retriever: LegalRetriever, generator: SelfRAGGenerator) -> dict:\n",
    "    \"\"\"\n",
    "    Generate answer with basic RAG (retrieve once, then generate).\n",
    "    \n",
    "    Args:\n",
    "        query: Legal question\n",
    "        retriever: Retrieval system\n",
    "        generator: Generator model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and metadata\n",
    "    \"\"\"\n",
    "    # Retrieve passages\n",
    "    retrieved_docs = retriever.retrieve(query, top_k=3)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        # Fallback to no-RAG\n",
    "        return no_rag_generation(query, generator)\n",
    "    \n",
    "    # Concatenate top passages\n",
    "    passages_text = \"\\n\\n\".join([doc['text'][:500] for doc in retrieved_docs[:2]])\n",
    "    \n",
    "    prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Relevant Legal Documents:\n",
    "{passages_text}\n",
    "\n",
    "Based on the provided documents, answer the legal question. Be concise and cite specific clauses when relevant.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    result = generator.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': result['generated_text'],\n",
    "        'method': 'Basic RAG',\n",
    "        'num_tokens': len(result['generated_text'].split()),\n",
    "        'retrieved_docs': [doc['source'] for doc in retrieved_docs],\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = basic_rag_generation(queries[0]['query'], retriever, generator)\n",
    "print(\"Basic RAG test:\")\n",
    "print(f\"  Answer: {test_result['answer'][:100]}...\")\n",
    "print(f\"  Retrieved from: {test_result['retrieved_docs'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag_generation(query: str, retriever: LegalRetriever, generator: SelfRAGGenerator, critic: Critic) -> dict:\n",
    "    \"\"\"\n",
    "    Generate answer with Self-RAG (adaptive retrieval + reflection tokens).\n",
    "    \n",
    "    Args:\n",
    "        query: Legal question\n",
    "        retriever: Retrieval system\n",
    "        generator: Self-RAG generator\n",
    "        critic: Critic model for reflection tokens\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer, reflection tokens, and metadata\n",
    "    \"\"\"\n",
    "    # Use Self-RAG pipeline\n",
    "    pipeline = SelfRAGPipeline(\n",
    "        generator=generator,\n",
    "        retriever=retriever,\n",
    "        critic=critic,\n",
    "    )\n",
    "    \n",
    "    result = pipeline.answer_question(\n",
    "        question=query,\n",
    "        include_retrieval=True,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': result['answer'],\n",
    "        'method': 'Self-RAG',\n",
    "        'num_tokens': len(result['answer'].split()),\n",
    "        'reflection_tokens': result.get('reflection_tokens', {}),\n",
    "        'score': result.get('score', 0.0),\n",
    "        'retrieved_docs': result.get('retrieved_docs', []),\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_result = self_rag_generation(queries[0]['query'], retriever, generator, critic)\n",
    "print(\"Self-RAG test:\")\n",
    "print(f\"  Answer: {test_result['answer'][:100]}...\")\n",
    "print(f\"  Reflection tokens: {test_result['reflection_tokens']}\")\n",
    "print(f\"  Score: {test_result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Self-RAG + INSIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag_inside_generation(query: str, retriever: LegalRetriever, generator: SelfRAGGenerator, critic: Critic) -> dict:\n",
    "    \"\"\"\n",
    "    Generate answer with Self-RAG + INSIDE hallucination detection.\n",
    "    \n",
    "    Args:\n",
    "        query: Legal question\n",
    "        retriever: Retrieval system\n",
    "        generator: Self-RAG generator with INSIDE enabled\n",
    "        critic: Critic model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer, reflection tokens, EigenScore, and metadata\n",
    "    \"\"\"\n",
    "    # Temporarily enable INSIDE\n",
    "    original_inside_enabled = generator.config.get('inside', {}).get('enabled', False)\n",
    "    \n",
    "    if 'inside' not in generator.config:\n",
    "        generator.config['inside'] = {}\n",
    "    generator.config['inside']['enabled'] = True\n",
    "    generator.config['inside']['extract_internal_states'] = True\n",
    "    \n",
    "    try:\n",
    "        # Use Self-RAG pipeline with INSIDE\n",
    "        pipeline = SelfRAGPipeline(\n",
    "            generator=generator,\n",
    "            retriever=retriever,\n",
    "            critic=critic,\n",
    "        )\n",
    "        \n",
    "        result = pipeline.answer_question(\n",
    "            question=query,\n",
    "            include_retrieval=True,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.0,\n",
    "            detect_hallucination=True,  # Enable INSIDE\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': result['answer'],\n",
    "            'method': 'Self-RAG+INSIDE',\n",
    "            'num_tokens': len(result['answer'].split()),\n",
    "            'reflection_tokens': result.get('reflection_tokens', {}),\n",
    "            'eigenscore': result.get('eigenscore', None),\n",
    "            'hallucination_detected': result.get('hallucination_detected', None),\n",
    "            'combined_score': result.get('combined_score', result.get('score', 0.0)),\n",
    "            'retrieved_docs': result.get('retrieved_docs', []),\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Restore original setting\n",
    "        generator.config['inside']['enabled'] = original_inside_enabled\n",
    "\n",
    "# Test\n",
    "test_result = self_rag_inside_generation(queries[0]['query'], retriever, generator, critic)\n",
    "print(\"Self-RAG+INSIDE test:\")\n",
    "print(f\"  Answer: {test_result['answer'][:100]}...\")\n",
    "print(f\"  EigenScore: {test_result.get('eigenscore', 'N/A')}\")\n",
    "print(f\"  Hallucination detected: {test_result.get('hallucination_detected', 'N/A')}\")\n",
    "print(f\"  Combined score: {test_result['combined_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation\n",
    "\n",
    "**Note:** This will take 1-2 hours for all 776 queries × 4 methods.\n",
    "For testing, use a smaller subset (e.g., 50 queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_SUBSET = True  # Set to False to run on full 776 queries\n",
    "SUBSET_SIZE = 50 if USE_SUBSET else len(queries)\n",
    "\n",
    "# Cache file\n",
    "CACHE_FILE = f\"../results/generation_results_{'subset' if USE_SUBSET else 'full'}.json\"\n",
    "Path(CACHE_FILE).parent.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Will evaluate on {SUBSET_SIZE} queries\")\n",
    "print(f\"Cache file: {CACHE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached results exist\n",
    "if Path(CACHE_FILE).exists():\n",
    "    print(f\"✓ Loading cached results from {CACHE_FILE}\")\n",
    "    with open(CACHE_FILE, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "    print(f\"  Loaded {len(all_results)} results\")\n",
    "else:\n",
    "    print(\"No cached results found. Will run evaluation...\")\n",
    "    all_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation (skip if cached)\n",
    "if all_results is None:\n",
    "    all_results = []\n",
    "    \n",
    "    for i, query_data in enumerate(tqdm(queries[:SUBSET_SIZE], desc=\"Evaluating\")):\n",
    "        query = query_data['query']\n",
    "        \n",
    "        # Get ground truth\n",
    "        if query_data['snippets']:\n",
    "            ground_truth = query_data['snippets'][0]['answer']\n",
    "        else:\n",
    "            ground_truth = \"\"\n",
    "        \n",
    "        result_entry = {\n",
    "            'query_id': i,\n",
    "            'query': query,\n",
    "            'ground_truth': ground_truth,\n",
    "            'dataset_source': query_data['dataset_source'],\n",
    "            'methods': {}\n",
    "        }\n",
    "        \n",
    "        # Method 1: No-RAG\n",
    "        try:\n",
    "            no_rag_result = no_rag_generation(query, generator)\n",
    "            result_entry['methods']['No-RAG'] = no_rag_result\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in No-RAG for query {i}: {e}\")\n",
    "            result_entry['methods']['No-RAG'] = {'error': str(e)}\n",
    "        \n",
    "        # Method 2: Basic RAG\n",
    "        try:\n",
    "            basic_rag_result = basic_rag_generation(query, retriever, generator)\n",
    "            result_entry['methods']['Basic RAG'] = basic_rag_result\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Basic RAG for query {i}: {e}\")\n",
    "            result_entry['methods']['Basic RAG'] = {'error': str(e)}\n",
    "        \n",
    "        # Method 3: Self-RAG\n",
    "        try:\n",
    "            self_rag_result = self_rag_generation(query, retriever, generator, critic)\n",
    "            result_entry['methods']['Self-RAG'] = self_rag_result\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Self-RAG for query {i}: {e}\")\n",
    "            result_entry['methods']['Self-RAG'] = {'error': str(e)}\n",
    "        \n",
    "        # Method 4: Self-RAG + INSIDE\n",
    "        try:\n",
    "            inside_result = self_rag_inside_generation(query, retriever, generator, critic)\n",
    "            result_entry['methods']['Self-RAG+INSIDE'] = inside_result\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Self-RAG+INSIDE for query {i}: {e}\")\n",
    "            result_entry['methods']['Self-RAG+INSIDE'] = {'error': str(e)}\n",
    "        \n",
    "        all_results.append(result_entry)\n",
    "    \n",
    "    # Save results\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to {CACHE_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.legalbench_generation_eval import (\n",
    "    evaluate_generation,\n",
    "    aggregate_metrics,\n",
    "    compare_methods,\n",
    ")\n",
    "\n",
    "# Compute metrics for each method\n",
    "metrics_by_method = {}\n",
    "\n",
    "for method_name in ['No-RAG', 'Basic RAG', 'Self-RAG', 'Self-RAG+INSIDE']:\n",
    "    method_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if method_name not in result['methods']:\n",
    "            continue\n",
    "        \n",
    "        method_data = result['methods'][method_name]\n",
    "        \n",
    "        if 'error' in method_data:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate this example\n",
    "        metrics = evaluate_generation(\n",
    "            prediction=method_data.get('answer', ''),\n",
    "            ground_truth=result['ground_truth'],\n",
    "            reflection_tokens=method_data.get('reflection_tokens'),\n",
    "            eigenscore=method_data.get('eigenscore'),\n",
    "        )\n",
    "        \n",
    "        method_results.append(metrics)\n",
    "    \n",
    "    # Aggregate\n",
    "    metrics_by_method[method_name] = aggregate_metrics(method_results)\n",
    "\n",
    "print(\"✓ Metrics computed for all methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for method_name, metrics in metrics_by_method.items():\n",
    "    row = {\n",
    "        'Method': method_name,\n",
    "        'F1': f\"{metrics['avg_f1_score']:.3f}\",\n",
    "        'ROUGE-L': f\"{metrics['avg_rouge_l']:.3f}\",\n",
    "        'Halluc%': f\"{metrics.get('hallucination_rate', 0) * 100:.1f}%\" if 'hallucination_rate' in metrics else 'N/A',\n",
    "        'Utility': f\"{metrics.get('avg_utility_score', 0):.2f}\" if 'avg_utility_score' in metrics else 'N/A',\n",
    "        'Avg Length': f\"{metrics['avg_prediction_length']:.0f}\",\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Generation Method Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: F1 and ROUGE-L comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "methods = list(metrics_by_method.keys())\n",
    "f1_scores = [metrics_by_method[m]['avg_f1_score'] for m in methods]\n",
    "rouge_scores = [metrics_by_method[m]['avg_rouge_l'] for m in methods]\n",
    "\n",
    "# F1 scores\n",
    "ax1.bar(methods, f1_scores, color=['#e74c3c', '#3498db', '#2ecc71', '#9b59b6'])\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('F1 Score by Method')\n",
    "ax1.set_ylim(0, max(f1_scores) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(f1_scores):\n",
    "    ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROUGE-L scores\n",
    "ax2.bar(methods, rouge_scores, color=['#e74c3c', '#3498db', '#2ecc71', '#9b59b6'])\n",
    "ax2.set_ylabel('ROUGE-L Score')\n",
    "ax2.set_title('ROUGE-L Score by Method')\n",
    "ax2.set_ylim(0, max(rouge_scores) * 1.2)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(rouge_scores):\n",
    "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/generation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved to results/generation_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart: Multi-metric comparison\n",
    "from math import pi\n",
    "\n",
    "# Prepare data (normalize to 0-1 for radar chart)\n",
    "categories = ['F1', 'ROUGE-L', 'Support\\n(1-Halluc)', 'Utility', 'Relevance']\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    metrics = metrics_by_method[method]\n",
    "    \n",
    "    values = [\n",
    "        metrics['avg_f1_score'],\n",
    "        metrics['avg_rouge_l'],\n",
    "        1 - metrics.get('hallucination_rate', 0.5),  # Invert hallucination\n",
    "        metrics.get('avg_utility_score', 0.5),\n",
    "        metrics.get('relevance_rate', 0.5),\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=method, color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.set_title('Multi-Metric Comparison (Radar Chart)', pad=20, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved to results/radar_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-subdataset breakdown\n",
    "subdataset_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for result in all_results:\n",
    "    dataset = result['dataset_source']\n",
    "    ground_truth = result['ground_truth']\n",
    "    \n",
    "    for method_name in methods:\n",
    "        if method_name not in result['methods']:\n",
    "            continue\n",
    "        \n",
    "        method_data = result['methods'][method_name]\n",
    "        \n",
    "        if 'error' in method_data:\n",
    "            continue\n",
    "        \n",
    "        # Compute F1\n",
    "        from evaluation.legalbench_generation_eval import compute_f1_score\n",
    "        f1 = compute_f1_score(method_data.get('answer', ''), ground_truth)\n",
    "        \n",
    "        subdataset_metrics[dataset][method_name].append(f1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (dataset, method_scores) in enumerate(sorted(subdataset_metrics.items())):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    \n",
    "    method_names = list(method_scores.keys())\n",
    "    avg_scores = [np.mean(method_scores[m]) for m in method_names]\n",
    "    \n",
    "    axes[i].bar(method_names, avg_scores, color=['#e74c3c', '#3498db', '#2ecc71', '#9b59b6'])\n",
    "    axes[i].set_title(f'{dataset} (n={len(method_scores[method_names[0]])})')\n",
    "    axes[i].set_ylabel('Avg F1 Score')\n",
    "    axes[i].set_ylim(0, max(avg_scores) * 1.2 if avg_scores else 1)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/subdataset_breakdown.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved to results/subdataset_breakdown.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Outputs\n",
    "\n",
    "Show side-by-side comparison of the same query across all 4 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an interesting example\n",
    "example_idx = 5\n",
    "example = all_results[example_idx]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Example Query (ID: {example['query_id']})\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset: {example['dataset_source']}\")\n",
    "print()\n",
    "print(f\"Query: {example['query']}\")\n",
    "print()\n",
    "print(f\"Ground Truth Snippet: {example['ground_truth'][:200]}...\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Method Outputs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method_name in methods:\n",
    "    if method_name not in example['methods']:\n",
    "        continue\n",
    "    \n",
    "    method_data = example['methods'][method_name]\n",
    "    \n",
    "    if 'error' in method_data:\n",
    "        print(f\"\\n{method_name}: ERROR - {method_data['error']}\")\n",
    "        continue\n",
    "    \n",
    "    answer = method_data.get('answer', '')\n",
    "    \n",
    "    # Compute metrics\n",
    "    from evaluation.legalbench_generation_eval import compute_f1_score, compute_rouge_l\n",
    "    f1 = compute_f1_score(answer, example['ground_truth'])\n",
    "    rouge = compute_rouge_l(answer, example['ground_truth'])\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  F1: {f1:.3f} | ROUGE-L: {rouge:.3f}\")\n",
    "    \n",
    "    if 'reflection_tokens' in method_data:\n",
    "        tokens = method_data['reflection_tokens']\n",
    "        print(f\"  Reflection: {tokens}\")\n",
    "    \n",
    "    if 'eigenscore' in method_data:\n",
    "        print(f\"  EigenScore: {method_data['eigenscore']:.2f}\")\n",
    "    \n",
    "    print(f\"  Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Analysis\n",
    "\n",
    "### Expected Results (with trained models):\n",
    "\n",
    "| Method | F1 | ROUGE-L | Halluc% | Utility | Notes |\n",
    "|--------|-----|---------|---------|---------|-------|\n",
    "| No-RAG | ~0.10-0.15 | ~0.12-0.18 | N/A | N/A | Baseline, no external knowledge |\n",
    "| Basic RAG | ~0.25-0.35 | ~0.30-0.40 | N/A | N/A | Simple retrieval helps significantly |\n",
    "| Self-RAG | ~0.40-0.50 | ~0.45-0.55 | ~15-25% | ~0.70-0.80 | Adaptive retrieval + self-assessment |\n",
    "| Self-RAG+INSIDE | ~0.42-0.52 | ~0.47-0.57 | ~10-18% | ~0.75-0.85 | Best overall, combines reflection + internal states |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **RAG is essential**: No-RAG performs poorly on legal questions requiring specific document knowledge\n",
    "2. **Self-reflection helps**: Self-RAG's reflection tokens enable self-assessment and adaptive retrieval\n",
    "3. **INSIDE reduces hallucinations**: EigenScore provides additional signal for hallucination detection\n",
    "4. **Trade-offs**: More sophisticated methods take longer but produce higher quality, more reliable answers\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Error Analysis**: Identify which types of queries each method struggles with\n",
    "2. **Qualitative Evaluation**: Manual review of outputs for legal correctness\n",
    "3. **Ablation Studies**: Test individual components (e.g., adaptive retrieval vs. fixed retrieval)\n",
    "4. **Hyperparameter Tuning**: Optimize reflection token weights, retrieval threshold, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
