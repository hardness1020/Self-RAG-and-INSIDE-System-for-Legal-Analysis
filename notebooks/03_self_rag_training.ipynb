{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Self-RAG Models\n",
    "\n",
    "Train critic and generator models using QLoRA for Self-RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before training:\n",
    "1. ✅ Documents indexed (from notebook 02)\n",
    "2. ✅ Training data prepared (from notebook 01)\n",
    "3. ⚠️ Training requires significant compute (GPU recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Training Labels\n",
    "\n",
    "Generate reflection token labels for Q&A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating labels: 100%|██████████| 10/10 [00:00<00:00, 45491.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled 10 examples\n",
      "Saved to ../data/training/labeled_data.json\n",
      "✅ Labels generated!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generate labels using rule-based approach\n",
    "uv run python -m src.training.generate_labels \\\n",
    "    --input ../data/samples/sample_qa_data.json \\\n",
    "    --output-dir ../data/training \\\n",
    "    --num-samples 10 && \\\n",
    "echo \"✅ Labels generated!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Critic Model\n",
    "\n",
    "Train the critic model to predict reflection tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CRITIC MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Configuration loaded from: ../configs/critic_config.yaml\n",
      "Resolved training_data_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training\n",
      "Resolved output_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/critic_lora\n",
      "\n",
      "============================================================\n",
      "Device Selection: MPS\n",
      "============================================================\n",
      "✓ Mac GPU (MPS) available and will be used\n",
      "  PyTorch MPS backend: True\n",
      "============================================================\n",
      "\n",
      "\n",
      "1. Loading tokenizer...\n",
      "   Added 18 reflection tokens to vocabulary\n",
      "\n",
      "2. Loading base model...\n",
      "   Note: 4-bit quantization disabled for macOS compatibility\n",
      "\n",
      "3. Preparing model for LoRA training...\n",
      "trainable params: 2,162,688 || all params: 495,968,768 || trainable%: 0.4361\n",
      "\n",
      "4. Loading and formatting training data...\n",
      "Loading training data from /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training/labeled_data.json\n",
      "Loaded 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting examples: 100%|██████████| 10/10 [00:00<00:00, 10462.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 40 training examples\n",
      "   Train: 36, Validation: 4\n",
      "\n",
      "5. Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 36/36 [00:00<00:00, 378.87 examples/s]\n",
      "Tokenizing validation: 100%|██████████| 4/4 [00:00<00:00, 2126.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Setting up training...\n",
      "\n",
      "7. Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 9/9 [01:42<00:00,  9.95s/it]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 103.2187, 'train_samples_per_second': 1.046, 'train_steps_per_second': 0.087, 'train_loss': 2.247092776828342, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:43<00:00,  9.95s/it]█| 9/9 [01:43<00:00, 11.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Saving final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "Model saved to: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/critic_lora/final\n",
      "================================================================================\n",
      "✅ Critic model trained!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Train critic (reduce epochs for testing)\n",
    "uv run python -m src.training.train_critic_qlora \\\n",
    "    --config ../configs/critic_config.yaml && \\\n",
    "echo \"✅ Critic model trained!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Generator Model\n",
    "\n",
    "Train the generator model with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATOR MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Configuration loaded from: ../configs/generator_config.yaml\n",
      "Resolved training_data_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training\n",
      "Resolved output_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/generator_lora\n",
      "\n",
      "============================================================\n",
      "Device Selection: MPS\n",
      "============================================================\n",
      "✓ Mac GPU (MPS) available and will be used\n",
      "  PyTorch MPS backend: True\n",
      "============================================================\n",
      "\n",
      "\n",
      "1. Loading tokenizer...\n",
      "   Added 18 reflection tokens to vocabulary\n",
      "\n",
      "2. Loading critic model from ../models/critic_lora/final...\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Warning: 4-bit quantization not supported on macOS. Loading model in full precision.\n",
      "Loading LoRA weights from ../models/critic_lora/final\n",
      "Model loaded successfully\n",
      "   Critic model loaded\n",
      "\n",
      "3. Loading training data...\n",
      "Loading training data from /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training/labeled_data.json\n",
      "Loaded 10 examples\n",
      "Augmenting data with critic model predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.40s/it]\n",
      "Formatting examples: 100%|██████████| 10/10 [00:00<00:00, 8005.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 10 training examples\n",
      "   Train: 9, Validation: 1\n",
      "\n",
      "4. Loading base model...\n",
      "   Note: 4-bit quantization disabled for macOS compatibility\n",
      "\n",
      "5. Preparing model for LoRA training...\n",
      "trainable params: 8,798,208 || all params: 502,604,288 || trainable%: 1.7505\n",
      "\n",
      "6. Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 9/9 [00:00<00:00, 1164.15 examples/s]\n",
      "Tokenizing validation: 100%|██████████| 1/1 [00:00<00:00, 541.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Setting up training...\n",
      "\n",
      "8. Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 3/3 [01:14<00:00, 24.82s/it]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 75.8859, 'train_samples_per_second': 0.356, 'train_steps_per_second': 0.04, 'train_loss': 2.025380770365397, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:15<00:00, 25.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Saving final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "Model saved to: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/generator_lora/final\n",
      "================================================================================\n",
      "✅ Generator model trained!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Train generator with critic weights\n",
    "uv run python -m src.training.train_generator_qlora \\\n",
    "    --config ../configs/generator_config.yaml \\\n",
    "    --critic-weights ../models/critic_lora/final && \\\n",
    "echo \"✅ Generator model trained!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Trained Models\n",
    "\n",
    "Quick test of the trained Self-RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Self-RAG Pipeline...\n",
      "\n",
      "1. Loading retriever...\n",
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on mps\n",
      "Embedding dimension: 768\n",
      "   Loading index from ../data/embeddings\n",
      "Using CPU index\n",
      "Created IndexFlatIP index with dimension 768\n",
      "Index loaded from ../data/embeddings/faiss_index.faiss\n",
      "Total documents in index: 10\n",
      "Documents loaded from ../data/embeddings/documents.pkl\n",
      "   Index loaded: 10 documents\n",
      "\n",
      "2. Loading generator...\n",
      "Loading generator model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Warning: 4-bit quantization not supported on macOS. Loading model in full precision.\n",
      "Loading LoRA weights from ../models/generator_lora/final\n",
      "Generator model loaded successfully\n",
      "MPS cache cleared\n",
      "\n",
      "3. Loading critic model for reflection tokens...\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Warning: 4-bit quantization not supported on macOS. Loading model in full precision.\n",
      "Loading LoRA weights from ../models/critic_lora/final\n",
      "Model loaded successfully\n",
      "MPS cache cleared\n",
      "   Critic model loaded successfully\n",
      "\n",
      "Pipeline loaded successfully!\n",
      "✅ Pipeline loaded!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.self_rag.inference import load_pipeline_from_config\n",
    "\n",
    "# Load complete pipeline\n",
    "pipeline = load_pipeline_from_config(\n",
    "    retrieval_config_path='../configs/retrieval_config.yaml',\n",
    "    generator_config_path='../configs/generator_config.yaml',\n",
    "    retriever_index_dir='../data/embeddings',\n",
    "    generator_weights_path='../models/generator_lora/final',\n",
    "    critic_weights_path='../models/critic_lora/final',\n",
    ")\n",
    "\n",
    "print(\"✅ Pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the elements of negligence?\n",
      "\n",
      "Answer: Negligence is a legal concept that involves a duty of care owed by one person to another. The duty of care requires that the defendant must have taken reasonable steps to prevent harm from occurring, and if they fail to do so, the plaintiff can recover damages for their injuries.\n",
      "The elements of negligence include:\n",
      "- Duty of care - A person owes a duty of care to others to avoid causing them harm.\n",
      "- Breach of duty - The defendant fails to take reasonable steps to prevent harm from occurring.\n",
      "- Damages - The plaintiff must be able to prove that they suffered actual harm as a result of the defendant's breach of duty.\n",
      "- Proximate cause - The defendant must have caused the harm in question proximately (directly) through their actions.\n",
      "- Contributory negligence - If the plaintiff was negligent themselves, they may be held responsible for any resulting damage.\n",
      "- Vicarious liability - If the defendant is acting within the scope of their employment or business relationship with the plaintiff, they may be held liable for the plaintiff's injury.\n",
      "- Comparative negligence - If the plaintiff is found to be at fault, they may be required to bear some or all of the responsibility for their own injuries.\n",
      "- Statute of limitations - Time limits apply for bringing a negligence claim, typically\n",
      "\n",
      "Reflection: {'retrieve': '[Retrieve]', 'isrel': None, 'issup': None, 'isuse': '[Utility:5]', 'intent': None}\n",
      "\n",
      "Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test question\n",
    "question = \"What are the elements of negligence?\"\n",
    "\n",
    "result = pipeline.answer_question(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\\n\")\n",
    "print(f\"Reflection: {result['reflection']}\\n\")\n",
    "print(f\"Score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tips\n",
    "\n",
    "### For CPU Training:\n",
    "- Reduce `per_device_train_batch_size` to 1-2\n",
    "- Increase `gradient_accumulation_steps`\n",
    "- Reduce `num_train_epochs` to 1 for testing\n",
    "- Use smaller models if available\n",
    "\n",
    "### For GPU Training:\n",
    "- Use larger batch sizes (4-8)\n",
    "- Enable `fp16` or `bf16` in config\n",
    "- Monitor GPU memory usage\n",
    "\n",
    "### Monitoring:\n",
    "- Check `models/*/logs/` for TensorBoard logs\n",
    "- Watch training loss decrease\n",
    "- Save checkpoints frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training complete!\n",
    "- ✅ Generated training labels\n",
    "- ✅ Trained critic model\n",
    "- ✅ Trained generator model\n",
    "- ✅ Tested Self-RAG pipeline\n",
    "\n",
    "**Next:** Proceed to `04_evaluation.ipynb` to evaluate performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag-legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
