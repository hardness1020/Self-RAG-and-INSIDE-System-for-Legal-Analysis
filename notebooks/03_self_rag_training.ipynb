{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Self-RAG Models\n",
    "\n",
    "Train critic and generator models using QLoRA for Self-RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before training:\n",
    "1. ✅ Documents indexed (from notebook 02)\n",
    "2. ✅ Training data prepared (from notebook 01)\n",
    "3. ⚠️ Training requires significant compute (GPU recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Training Labels\n",
    "\n",
    "Generate reflection token labels for Q&A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(46831) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating labels: 100%|██████████| 10/10 [00:00<00:00, 29495.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled 10 examples\n",
      "Saved to ../data/training/labeled_data.json\n",
      "✅ Labels generated!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generate labels using rule-based approach\n",
    "uv run python -m src.training.generate_labels \\\n",
    "    --input ../data/samples/sample_qa_data.json \\\n",
    "    --output-dir ../data/training \\\n",
    "    --num-samples 10 && \\\n",
    "echo \"✅ Labels generated!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Critic Model\n",
    "\n",
    "Train the critic model to predict reflection tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(46835) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CRITIC MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Configuration loaded from: ../configs/critic_config.yaml\n",
      "Resolved training_data_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training\n",
      "Resolved output_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/critic_lora\n",
      "\n",
      "============================================================\n",
      "Device Selection: MPS\n",
      "============================================================\n",
      "✓ Mac GPU (MPS) available and will be used\n",
      "  PyTorch MPS backend: True\n",
      "============================================================\n",
      "\n",
      "\n",
      "1. Loading tokenizer...\n",
      "   Added 18 reflection tokens to vocabulary\n",
      "\n",
      "2. Loading base model...\n",
      "   Note: 4-bit quantization disabled for macOS compatibility\n",
      "\n",
      "3. Preparing model for LoRA training...\n",
      "trainable params: 4,358,144 || all params: 1,547,683,840 || trainable%: 0.2816\n",
      "\n",
      "4. Loading and formatting training data...\n",
      "Loading training data from /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training/labeled_data.json\n",
      "Loaded 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting examples: 100%|██████████| 10/10 [00:00<00:00, 3107.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 40 training examples\n",
      "   Train: 36, Validation: 4\n",
      "\n",
      "5. Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 36/36 [00:00<00:00, 2436.86 examples/s]\n",
      "Tokenizing validation: 100%|██████████| 4/4 [00:00<00:00, 1769.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Setting up training...\n",
      "\n",
      "7. Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 9/9 [07:13<00:00, 39.90s/it]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "100%|██████████| 9/9 [07:17<00:00, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 437.7311, 'train_samples_per_second': 0.247, 'train_steps_per_second': 0.021, 'train_loss': 2.1289632585313587, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [07:17<00:00, 48.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Saving final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "Model saved to: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/critic_lora/final\n",
      "================================================================================\n",
      "✅ Critic model trained!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Train critic (reduce epochs for testing)\n",
    "uv run python -m src.training.train_critic_qlora \\\n",
    "    --config ../configs/critic_config.yaml && \\\n",
    "echo \"✅ Critic model trained!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Generator Model\n",
    "\n",
    "Train the generator model with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(47727) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATOR MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Configuration loaded from: ../configs/generator_config.yaml\n",
      "Resolved training_data_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training\n",
      "Resolved output_dir: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/generator_lora\n",
      "\n",
      "============================================================\n",
      "Device Selection: MPS\n",
      "============================================================\n",
      "✓ Mac GPU (MPS) available and will be used\n",
      "  PyTorch MPS backend: True\n",
      "============================================================\n",
      "\n",
      "\n",
      "1. Loading tokenizer...\n",
      "   Added 18 reflection tokens to vocabulary\n",
      "\n",
      "2. Loading critic model from ../models/critic_lora/final...\n",
      "Loading model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Warning: 4-bit quantization not supported on macOS. Loading model in full precision.\n",
      "Loading LoRA weights from ../models/critic_lora/final\n",
      "Model loaded successfully\n",
      "   Critic model loaded\n",
      "\n",
      "3. Loading training data...\n",
      "Loading training data from /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/data/training/labeled_data.json\n",
      "Loaded 10 examples\n",
      "Augmenting data with critic model predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.68s/it]\n",
      "Formatting examples: 100%|██████████| 10/10 [00:00<00:00, 8633.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 10 training examples\n",
      "   Train: 9, Validation: 1\n",
      "\n",
      "4. Loading base model...\n",
      "   Note: 4-bit quantization disabled for macOS compatibility\n",
      "\n",
      "5. Preparing model for LoRA training...\n",
      "trainable params: 18,464,768 || all params: 1,561,790,464 || trainable%: 1.1823\n",
      "\n",
      "6. Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 9/9 [00:00<00:00, 325.65 examples/s]\n",
      "Tokenizing validation: 100%|██████████| 1/1 [00:00<00:00, 651.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Setting up training...\n",
      "\n",
      "8. Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 3/3 [04:14<00:00, 86.22s/it]/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [04:18<00:0             0, 86.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 258.7341, 'train_samples_per_second': 0.104, 'train_steps_per_second': 0.012, 'train_loss': 1.7339324951171875, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:18<00:00, 86.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Saving final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "Model saved to: /Users/marcuschang/Library/CloudStorage/OneDrive-Personal/桌面/UCSD/DSC261_Responsible_DS/models/generator_lora/final\n",
      "================================================================================\n",
      "✅ Generator model trained!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Train generator with critic weights\n",
    "uv run python -m src.training.train_generator_qlora \\\n",
    "    --config ../configs/generator_config.yaml \\\n",
    "    --critic-weights ../models/critic_lora/final && \\\n",
    "echo \"✅ Generator model trained!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Trained Models\n",
    "\n",
    "Quick test of the trained Self-RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Self-RAG Pipeline...\n",
      "\n",
      "1. Loading retriever...\n",
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on mps\n",
      "Embedding dimension: 768\n",
      "   Loading index from ../data/embeddings\n",
      "Using CPU index\n",
      "Created IndexFlatIP index with dimension 768\n",
      "Index loaded from ../data/embeddings/faiss_index.faiss\n",
      "Total documents in index: 10\n",
      "Documents loaded from ../data/embeddings/documents.pkl\n",
      "   Index loaded: 10 documents\n",
      "\n",
      "2. Loading generator...\n",
      "Loading generator model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Warning: 4-bit quantization not supported on macOS. Loading model in full precision.\n",
      "Loading LoRA weights from ../models/generator_lora/final\n",
      "Generator model loaded successfully\n",
      "\n",
      "3. Loading critic model for reflection tokens...\n",
      "   Warning: Could not load critic model: [Errno 2] No such file or directory: 'configs/critic_config.yaml'\n",
      "   Continuing without critic - reflection tokens may be unavailable\n",
      "\n",
      "Pipeline loaded successfully!\n",
      "✅ Pipeline loaded!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.self_rag.inference import load_pipeline_from_config\n",
    "\n",
    "# Load complete pipeline\n",
    "pipeline = load_pipeline_from_config(\n",
    "    retrieval_config_path='../configs/retrieval_config.yaml',\n",
    "    generator_config_path='../configs/generator_config.yaml',\n",
    "    retriever_index_dir='../data/embeddings',\n",
    "    generator_weights_path='../models/generator_lora/final',\n",
    "    critic_weights_path='../models/critic_lora/final',\n",
    ")\n",
    "\n",
    "print(\"✅ Pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the elements of negligence?\n",
      "\n",
      "Answer: The four elements of negligence are:\n",
      "1. A duty to act.\n",
      "2. A breach of that duty.\n",
      "3. An injury or damage caused by the breach.\n",
      "4. Proximate cause, which means that the injury was reasonably foreseeable from the defendant's conduct and proximately resulted from it.\n",
      "Negligence is a legal concept that allows people who have not acted intentionally to be held responsible for harm they cause due to their failure to exercise reasonable care. In other words, if someone fails to take reasonable precautions to prevent harm, and as a result causes an injury, that person may be liable in tort (which means \"wrongful\") for that injury.\n",
      "\n",
      "The first element of negligence is a duty to act. This requires that there be some\n",
      "\n",
      "Reflection: {'retrieve': None, 'isrel': None, 'issup': None, 'isuse': None, 'intent': None}\n",
      "\n",
      "Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test question\n",
    "question = \"What are the elements of negligence?\"\n",
    "\n",
    "result = pipeline.answer_question(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {result['answer']}\\n\")\n",
    "print(f\"Reflection: {result['reflection']}\\n\")\n",
    "print(f\"Score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tips\n",
    "\n",
    "### For CPU Training:\n",
    "- Reduce `per_device_train_batch_size` to 1-2\n",
    "- Increase `gradient_accumulation_steps`\n",
    "- Reduce `num_train_epochs` to 1 for testing\n",
    "- Use smaller models if available\n",
    "\n",
    "### For GPU Training:\n",
    "- Use larger batch sizes (4-8)\n",
    "- Enable `fp16` or `bf16` in config\n",
    "- Monitor GPU memory usage\n",
    "\n",
    "### Monitoring:\n",
    "- Check `models/*/logs/` for TensorBoard logs\n",
    "- Watch training loss decrease\n",
    "- Save checkpoints frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training complete!\n",
    "- ✅ Generated training labels\n",
    "- ✅ Trained critic model\n",
    "- ✅ Trained generator model\n",
    "- ✅ Tested Self-RAG pipeline\n",
    "\n",
    "**Next:** Proceed to `04_evaluation.ipynb` to evaluate performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag-legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
