{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Training Self-RAG Models on LegalBench\n",
    "\n",
    "**Objective:** Train generator and critic models for Self-RAG using LegalBench training labels\n",
    "\n",
    "**Training Components:**\n",
    "1. **Generator**: Fine-tune with LoRA to generate answers with reflection tokens\n",
    "2. **Critic**: Fine-tune with LoRA to predict reflection tokens (ISREL, ISSUP, ISUSE)\n",
    "\n",
    "**Training Data:** LegalBench mini (776 queries) with labels generated by Qwen2.5-7B-Instruct\n",
    "\n",
    "**Expected Training Time:** 2-3 hours (depends on hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete\n",
      "PyTorch version: 2.9.0\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Set seeds\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Setup complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Training Labels\n",
    "\n",
    "This cell generates reflection token labels for 776 LegalBench queries using Qwen2.5-7B-Instruct.\n",
    "\n",
    "**Process:**\n",
    "1. Loads queries from LegalBench-RAG benchmark\n",
    "2. Retrieves relevant passages using existing FAISS index\n",
    "3. Generates labels: ISREL, ISSUP, ISUSE, Retrieve tokens\n",
    "4. Saves to `data/training/legalbench_training_labels.json`\n",
    "\n",
    "**Runtime:** ~1-2 hours on first run (model downloads ~7GB if not cached)\n",
    "\n",
    "**Note:** This cell automatically skips if labels already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training labels...\n",
      "================================================================================\n",
      "Using device: mps\n",
      "\n",
      "Loaded 776 queries from LegalBench-RAG\n",
      "\n",
      "Loading retriever from ../data/legalbench_embeddings...\n",
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on cpu\n",
      "Embedding dimension: 768\n",
      "Using CPU index\n",
      "Created IndexFlatIP index with dimension 768\n",
      "Index loaded from ../data/legalbench_embeddings/faiss_index.faiss\n",
      "Total documents in index: 326783\n",
      "Documents loaded from ../data/legalbench_embeddings/documents.pkl\n",
      "âœ“ Retriever loaded\n",
      "\n",
      "Loading labeling model: Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4eb2eee2d4429780cf2c19a68bcc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d040572e324001b9a0b7bb06beb50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75604b9962b24a1c8701fb2b1058174d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded on mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507af8f9df1848f99548bc51d5bf727b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating labels:   0%|          | 0/776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# Generate training labels if they don't exist\n",
    "LABELS_FILE = \"../data/training/legalbench_training_labels.json\"\n",
    "\n",
    "if Path(LABELS_FILE).exists():\n",
    "    print(f\"âœ“ Labels already exist at {LABELS_FILE}\")\n",
    "    print(\"  Skipping generation. Delete the file to regenerate.\")\n",
    "else:\n",
    "    print(\"Generating training labels...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Determine device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    # Load LegalBench queries\n",
    "    QUERIES_FILE = \"../data/legalbench-rag/queries.json\"\n",
    "    with open(QUERIES_FILE, 'r') as f:\n",
    "        all_queries = json.load(f)\n",
    "    \n",
    "    # Use first 776 queries (mini dataset)\n",
    "    queries = all_queries['tests'][:776]\n",
    "    print(f\"Loaded {len(queries)} queries from LegalBench-RAG\")\n",
    "    \n",
    "    # Load retriever for passage retrieval\n",
    "    from retrieval.retriever import LegalRetriever\n",
    "    from retrieval.embedding import EmbeddingModel\n",
    "    \n",
    "    INDEX_DIR = \"../data/legalbench_embeddings\"\n",
    "    print(f\"\\nLoading retriever from {INDEX_DIR}...\")\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    embedding_model = EmbeddingModel(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    # Initialize retriever\n",
    "    retriever = LegalRetriever(\n",
    "        embedding_model=embedding_model,\n",
    "        top_k=3\n",
    "    )\n",
    "    \n",
    "    # Load pre-built index\n",
    "    retriever.load_index(INDEX_DIR)\n",
    "    print(\"âœ“ Retriever loaded\")\n",
    "    \n",
    "    # Load labeling model\n",
    "    LABEL_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    print(f\"\\nLoading labeling model: {LABEL_MODEL}...\")\n",
    "    \n",
    "    label_tokenizer = AutoTokenizer.from_pretrained(LABEL_MODEL)\n",
    "    label_model = AutoModelForCausalLM.from_pretrained(\n",
    "        LABEL_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=device if device != \"mps\" else None,\n",
    "    )\n",
    "    \n",
    "    if device == \"mps\":\n",
    "        label_model = label_model.to(\"mps\")\n",
    "    \n",
    "    print(f\"âœ“ Model loaded on {device}\")\n",
    "    \n",
    "    # Generate labels for each query\n",
    "    training_labels = []\n",
    "    \n",
    "    for query_data in tqdm(queries, desc=\"Generating labels\"):\n",
    "        query = query_data['query']\n",
    "        ground_truth = query_data.get('answer', '')\n",
    "        dataset_source = query_data.get('dataset', 'unknown')\n",
    "        \n",
    "        # Retrieve passages\n",
    "        try:\n",
    "            results = retriever.retrieve(query, top_k=5)\n",
    "            top_passage = results[0]['text'] if results else \"\"\n",
    "        except:\n",
    "            top_passage = \"\"\n",
    "        \n",
    "        # Generate reflection tokens using the labeling model\n",
    "        prompt = f\"\"\"Given a legal question, retrieved passage, and answer, predict reflection tokens.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Passage: {top_passage[:500]}\n",
    "\n",
    "Answer: {ground_truth}\n",
    "\n",
    "Predict the following tokens:\n",
    "- ISREL (Is the passage relevant?): [Relevant] or [Irrelevant]\n",
    "- ISSUP (Does the passage support the answer?): [Fully supported] or [Partially supported] or [No support]\n",
    "- ISUSE (Should this answer be used?): [5] (best) to [1] (worst)\n",
    "\n",
    "Reflection tokens:\"\"\"\n",
    "        \n",
    "        inputs = label_tokenizer(prompt, return_tensors=\"pt\").to(label_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = label_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=label_tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = label_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse reflection tokens from response\n",
    "        reflection_tokens = {}\n",
    "        \n",
    "        # Simple parsing - extract tokens from response\n",
    "        if \"[Relevant]\" in response:\n",
    "            reflection_tokens['isrel'] = \"[Relevant]\"\n",
    "        elif \"[Irrelevant]\" in response:\n",
    "            reflection_tokens['isrel'] = \"[Irrelevant]\"\n",
    "        \n",
    "        if \"[Fully supported]\" in response:\n",
    "            reflection_tokens['issup'] = \"[Fully supported]\"\n",
    "        elif \"[Partially supported]\" in response:\n",
    "            reflection_tokens['issup'] = \"[Partially supported]\"\n",
    "        elif \"[No support]\" in response:\n",
    "            reflection_tokens['issup'] = \"[No support]\"\n",
    "        \n",
    "        # Extract ISUSE score\n",
    "        for score in ['[5]', '[4]', '[3]', '[2]', '[1]']:\n",
    "            if score in response:\n",
    "                reflection_tokens['isuse'] = score\n",
    "                break\n",
    "        \n",
    "        # Always retrieve for training\n",
    "        reflection_tokens['retrieve'] = \"[Retrieve]\"\n",
    "        \n",
    "        # Store training example\n",
    "        training_labels.append({\n",
    "            'question': query,\n",
    "            'passage': top_passage,\n",
    "            'answer': ground_truth,\n",
    "            'reflection_tokens': reflection_tokens,\n",
    "            'dataset_source': dataset_source,\n",
    "        })\n",
    "    \n",
    "    # Save labels\n",
    "    Path(LABELS_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(LABELS_FILE, 'w') as f:\n",
    "        json.dump(training_labels, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Generated {len(training_labels)} training labels\")\n",
    "    print(f\"âœ“ Saved to {LABELS_FILE}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del label_model, label_tokenizer\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nâœ“ Label generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load and Inspect Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "LABELS_FILE = \"../data/training/legalbench_training_labels.json\"\n",
    "\n",
    "if not Path(LABELS_FILE).exists():\n",
    "    print(f\"âš ï¸  Labels file not found: {LABELS_FILE}\")\n",
    "    print(\"   This should have been generated in the previous cell.\")\n",
    "    print(\"   Please run the previous cell to generate labels.\")\n",
    "    raise FileNotFoundError(LABELS_FILE)\n",
    "\n",
    "with open(LABELS_FILE, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} training examples\")\n",
    "print(f\"\\nExample structure:\")\n",
    "print(json.dumps(training_data[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Label Distribution Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "token_types = ['retrieve', 'isrel', 'issup', 'isuse']\n",
    "\n",
    "for token_type in token_types:\n",
    "    if any(token_type in ex['reflection_tokens'] for ex in training_data):\n",
    "        values = [ex['reflection_tokens'].get(token_type, 'N/A') for ex in training_data]\n",
    "        counter = Counter(values)\n",
    "        \n",
    "        print(f\"\\n{token_type.upper()}:\")\n",
    "        for val, count in counter.most_common():\n",
    "            if val != 'N/A':\n",
    "                pct = (count / len(values)) * 100\n",
    "                print(f\"  {val}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Dataset source distribution\n",
    "source_counter = Counter(ex['dataset_source'] for ex in training_data)\n",
    "print(f\"\\nDataset Sources:\")\n",
    "for source, count in source_counter.most_common():\n",
    "    print(f\"  {source}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, token_type in enumerate(token_types):\n",
    "    values = [ex['reflection_tokens'].get(token_type, 'N/A') for ex in training_data]\n",
    "    counter = Counter(v for v in values if v != 'N/A')\n",
    "    \n",
    "    labels = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "    \n",
    "    axes[i].bar(range(len(labels)), counts, color='steelblue')\n",
    "    axes[i].set_xticks(range(len(labels)))\n",
    "    axes[i].set_xticklabels([l[:20] for l in labels], rotation=45, ha='right')\n",
    "    axes[i].set_title(f'{token_type.upper()} Distribution')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/training_label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/training_label_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split (80/20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    training_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[ex['dataset_source'] for ex in training_data]  # Stratify by dataset\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nTrain dataset distribution:\")\n",
    "train_sources = Counter(ex['dataset_source'] for ex in train_data)\n",
    "for source, count in train_sources.most_common():\n",
    "    print(f\"  {source}: {count}\")\n",
    "\n",
    "print(\"\\nValidation dataset distribution:\")\n",
    "val_sources = Counter(ex['dataset_source'] for ex in val_data)\n",
    "for source, count in val_sources.most_common():\n",
    "    print(f\"  {source}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Format Data for Generator Training\n",
    "\n",
    "Generator learns to produce: `answer [ISSUP] [ISREL] [ISUSE]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_generator_example(example):\n",
    "    \"\"\"\n",
    "    Format example for generator training.\n",
    "    \n",
    "    Input: question + passage\n",
    "    Output: answer with reflection tokens\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "    passage = example.get('passage', '')[:500]  # Limit passage length\n",
    "    answer = example.get('answer', '')\n",
    "    \n",
    "    tokens = example['reflection_tokens']\n",
    "    \n",
    "    # Build input\n",
    "    if passage:\n",
    "        input_text = f\"\"\"Question: {question}\n",
    "\n",
    "Passage: {passage}\n",
    "\n",
    "Answer with reflection tokens:\"\"\"\n",
    "    else:\n",
    "        input_text = f\"\"\"Question: {question}\n",
    "\n",
    "Answer with reflection tokens:\"\"\"\n",
    "    \n",
    "    # Build output with reflection tokens\n",
    "    output_text = answer\n",
    "    \n",
    "    if 'issup' in tokens:\n",
    "        output_text += f\" {tokens['issup']}\"\n",
    "    if 'isrel' in tokens:\n",
    "        output_text += f\" {tokens['isrel']}\"\n",
    "    if 'isuse' in tokens:\n",
    "        output_text += f\" {tokens['isuse']}\"\n",
    "    \n",
    "    return {\n",
    "        'input': input_text,\n",
    "        'output': output_text,\n",
    "        'full_text': input_text + \"\\n\" + output_text,\n",
    "    }\n",
    "\n",
    "# Format train and validation data\n",
    "generator_train = [format_generator_example(ex) for ex in train_data]\n",
    "generator_val = [format_generator_example(ex) for ex in val_data]\n",
    "\n",
    "print(\"Generator Training Example:\")\n",
    "print(\"=\" * 80)\n",
    "print(generator_train[0]['full_text'][:500] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Format Data for Critic Training\n",
    "\n",
    "Critic learns to predict: `[ISREL] [ISSUP] [ISUSE]` given question + passage + answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_critic_example(example):\n",
    "    \"\"\"\n",
    "    Format example for critic training.\n",
    "    \n",
    "    Input: question + passage + answer\n",
    "    Output: reflection tokens\n",
    "    \"\"\"\n",
    "    question = example['question']\n",
    "    passage = example.get('passage', '')[:500]\n",
    "    answer = example.get('answer', '')\n",
    "    \n",
    "    tokens = example['reflection_tokens']\n",
    "    \n",
    "    # Build input\n",
    "    input_text = f\"\"\"Question: {question}\n",
    "\n",
    "Passage: {passage}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Predict reflection tokens:\"\"\"\n",
    "    \n",
    "    # Build output (only reflection tokens)\n",
    "    output_tokens = []\n",
    "    if 'isrel' in tokens:\n",
    "        output_tokens.append(tokens['isrel'])\n",
    "    if 'issup' in tokens:\n",
    "        output_tokens.append(tokens['issup'])\n",
    "    if 'isuse' in tokens:\n",
    "        output_tokens.append(tokens['isuse'])\n",
    "    \n",
    "    output_text = \" \".join(output_tokens)\n",
    "    \n",
    "    return {\n",
    "        'input': input_text,\n",
    "        'output': output_text,\n",
    "        'full_text': input_text + \"\\n\" + output_text,\n",
    "    }\n",
    "\n",
    "# Format train and validation data\n",
    "critic_train = [format_critic_example(ex) for ex in train_data]\n",
    "critic_val = [format_critic_example(ex) for ex in val_data]\n",
    "\n",
    "print(\"Critic Training Example:\")\n",
    "print(\"=\" * 80)\n",
    "print(critic_train[0]['full_text'][:500] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load Base Model and Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Small, fast for training\n",
    "GENERATOR_OUTPUT_DIR = \"../models/generator_legalbench_lora\"\n",
    "\n",
    "# Create output directory\n",
    "Path(GENERATOR_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if device != \"cpu\" else torch.float32,\n",
    "    device_map=device if device != \"mps\" else None,\n",
    ")\n",
    "\n",
    "if device == \"mps\":\n",
    "    model = model.to(\"mps\")\n",
    "\n",
    "print(f\"âœ“ Model loaded on {device}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # Scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Tokenize examples for training.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        examples['full_text'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # Labels are same as input_ids for causal LM\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(generator_train)\n",
    "val_dataset = Dataset.from_list(generator_val)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer),\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer),\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Datasets prepared\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=GENERATOR_OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=False,  # Use bf16 if supported\n",
    "    bf16=True if device == \"cuda\" else False,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size} x {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Total steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting generator training...\")\n",
    "print(\"This will take ~1-2 hours\")\n",
    "print()\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime'] / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(GENERATOR_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(GENERATOR_OUTPUT_DIR)\n",
    "\n",
    "print(f\"âœ“ Model saved to {GENERATOR_OUTPUT_DIR}\")\n",
    "\n",
    "# Save training metrics\n",
    "import pandas as pd\n",
    "\n",
    "training_logs = pd.DataFrame(trainer.state.log_history)\n",
    "training_logs.to_csv(f\"{GENERATOR_OUTPUT_DIR}/training_logs.csv\", index=False)\n",
    "print(f\"âœ“ Training logs saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# Training loss\n",
    "train_logs = logs[logs['loss'].notna()]\n",
    "ax.plot(train_logs['step'], train_logs['loss'], label='Train Loss', linewidth=2)\n",
    "\n",
    "# Validation loss\n",
    "eval_logs = logs[logs['eval_loss'].notna()]\n",
    "ax.plot(eval_logs['step'], eval_logs['eval_loss'], label='Validation Loss', linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Generator Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/generator_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/generator_training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Test Generator on Validation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few validation examples\n",
    "print(\"Testing generator on validation examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(3):\n",
    "    example = generator_val[i]\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(example['input'], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {example['input'][:100]}...\")\n",
    "    print(f\"\\nExpected: {example['output']}\")\n",
    "    print(f\"\\nGenerated: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Critic Training\n",
    "\n",
    "Now train the critic to predict reflection tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleared memory for critic training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load Base Model and Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CRITIC_OUTPUT_DIR = \"../models/critic_legalbench_lora\"\n",
    "Path(CRITIC_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading model for critic: {MODEL_NAME}...\")\n",
    "\n",
    "# Load fresh model\n",
    "critic_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if device != \"cpu\" else torch.float32,\n",
    "    device_map=device if device != \"mps\" else None,\n",
    ")\n",
    "\n",
    "if device == \"mps\":\n",
    "    critic_model = critic_model.to(\"mps\")\n",
    "\n",
    "# Apply LoRA\n",
    "critic_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "critic_model = get_peft_model(critic_model, critic_lora_config)\n",
    "critic_model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ Critic model configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Critic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "critic_train_dataset = Dataset.from_list(critic_train)\n",
    "critic_val_dataset = Dataset.from_list(critic_val)\n",
    "\n",
    "# Tokenize\n",
    "critic_train_dataset = critic_train_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, max_length=512),\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing critic train\"\n",
    ")\n",
    "\n",
    "critic_val_dataset = critic_val_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, max_length=512),\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing critic validation\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Critic datasets prepared\")\n",
    "print(f\"  Train: {len(critic_train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(critic_val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for critic\n",
    "critic_training_args = TrainingArguments(\n",
    "    output_dir=CRITIC_OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=False,\n",
    "    bf16=True if device == \"cuda\" else False,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "critic_trainer = Trainer(\n",
    "    model=critic_model,\n",
    "    args=critic_training_args,\n",
    "    train_dataset=critic_train_dataset,\n",
    "    eval_dataset=critic_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting critic training...\")\n",
    "print(\"This will take ~30-60 minutes\")\n",
    "print()\n",
    "\n",
    "# Train\n",
    "critic_train_result = critic_trainer.train()\n",
    "\n",
    "print(\"\\nâœ“ Critic training complete!\")\n",
    "print(f\"  Final train loss: {critic_train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Training time: {critic_train_result.metrics['train_runtime'] / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save critic model\n",
    "critic_trainer.save_model(CRITIC_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(CRITIC_OUTPUT_DIR)\n",
    "\n",
    "print(f\"âœ“ Critic model saved to {CRITIC_OUTPUT_DIR}\")\n",
    "\n",
    "# Save training logs\n",
    "critic_logs = pd.DataFrame(critic_trainer.state.log_history)\n",
    "critic_logs.to_csv(f\"{CRITIC_OUTPUT_DIR}/training_logs.csv\", index=False)\n",
    "print(f\"âœ“ Critic training logs saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Critic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot critic loss curves\n",
    "critic_logs_df = pd.DataFrame(critic_trainer.state.log_history)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# Training loss\n",
    "train_logs = critic_logs_df[critic_logs_df['loss'].notna()]\n",
    "ax.plot(train_logs['step'], train_logs['loss'], label='Train Loss', linewidth=2)\n",
    "\n",
    "# Validation loss\n",
    "eval_logs = critic_logs_df[critic_logs_df['eval_loss'].notna()]\n",
    "ax.plot(eval_logs['step'], eval_logs['eval_loss'], label='Validation Loss', linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Critic Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/critic_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/critic_training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Test Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test critic on validation examples\n",
    "print(\"Testing critic on validation examples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "critic_model.eval()\n",
    "\n",
    "for i in range(3):\n",
    "    example = critic_val[i]\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(example['input'], return_tensors=\"pt\").to(critic_model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = critic_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Expected tokens: {example['output']}\")\n",
    "    print(f\"Predicted tokens: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š Generator:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Training examples: {len(train_data)}\")\n",
    "print(f\"  Validation examples: {len(val_data)}\")\n",
    "print(f\"  Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime'] / 60:.1f} minutes\")\n",
    "print(f\"  Saved to: {GENERATOR_OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Critic:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Training examples: {len(train_data)}\")\n",
    "print(f\"  Validation examples: {len(val_data)}\")\n",
    "print(f\"  Final train loss: {critic_train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"  Training time: {critic_train_result.metrics['train_runtime'] / 60:.1f} minutes\")\n",
    "print(f\"  Saved to: {CRITIC_OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Next Steps\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Update model configs:\")\n",
    "print(f\"   - configs/generator_config.yaml\")\n",
    "print(f\"   - configs/critic_config.yaml\")\n",
    "print(\"\\n2. Run full evaluation:\")\n",
    "print(f\"   - Open notebooks/11_generation_evaluation.ipynb\")\n",
    "print(f\"   - Run evaluation on 776 queries\")\n",
    "print(\"\\n3. Compare methods:\")\n",
    "print(f\"   - No-RAG vs Basic RAG vs Self-RAG vs Self-RAG+INSIDE\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to file\n",
    "summary = {\n",
    "    'generator': {\n",
    "        'model': MODEL_NAME,\n",
    "        'train_examples': len(train_data),\n",
    "        'val_examples': len(val_data),\n",
    "        'final_train_loss': float(train_result.metrics['train_loss']),\n",
    "        'training_time_minutes': float(train_result.metrics['train_runtime'] / 60),\n",
    "        'output_dir': GENERATOR_OUTPUT_DIR,\n",
    "    },\n",
    "    'critic': {\n",
    "        'model': MODEL_NAME,\n",
    "        'train_examples': len(train_data),\n",
    "        'val_examples': len(val_data),\n",
    "        'final_train_loss': float(critic_train_result.metrics['train_loss']),\n",
    "        'training_time_minutes': float(critic_train_result.metrics['train_runtime'] / 60),\n",
    "        'output_dir': CRITIC_OUTPUT_DIR,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Training summary saved to results/training_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag-legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
