{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Pre-trained Self-RAG with GGUF Conversion\n",
    "\n",
    "This notebook uses the official pre-trained Self-RAG model (7B) converted to GGUF format for efficient inference on Mac M4 (16GB).\n",
    "\n",
    "## Why This Approach\n",
    "- Pre-trained Self-RAG already learned reflection tokens on 150k examples\n",
    "- GGUF quantization enables running 7B model on 16GB Mac\n",
    "\n",
    "## Sections\n",
    "1. Setup & Dependencies\n",
    "2. Download Pre-trained Self-RAG Model\n",
    "3. Convert to GGUF Format\n",
    "4. Validate Special Tokens\n",
    "5. Inference Integration\n",
    "6. Integrate with Existing Retriever\n",
    "7. Evaluation on LegalBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Install dependencies\n",
    "# For Mac with Metal acceleration:\n",
    "# CMAKE_ARGS=\"-DLLAMA_METAL=on\" uv pip install llama-cpp-python --upgrade\n",
    "\n",
    "# Uncomment to install:\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" uv pip install llama-cpp-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.2: Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Existing project imports\n",
    "from src.retrieval.retriever import LegalRetriever\n",
    "from src.retrieval.embedding import EmbeddingModel\n",
    "from src.self_rag.reflection_tokens import ReflectionTokenizer, ReflectionAnnotation\n",
    "from src.self_rag.gguf_inference import SelfRAGGGUFInference, SelfRAGOutput\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-cpp-python version: 0.3.16\n",
      "Metal acceleration: Available on macOS\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.3: Verify llama-cpp-python setup\n",
    "try:\n",
    "    import llama_cpp\n",
    "    print(f\"llama-cpp-python version: {llama_cpp.__version__}\")\n",
    "    print(\"Metal acceleration: Available on macOS\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: llama-cpp-python not installed!\")\n",
    "    print(\"Run: CMAKE_ARGS='-DLLAMA_METAL=on' uv pip install llama-cpp-python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Download Pre-trained Self-RAG Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites - Llama 2 License\n",
    "\n",
    "**IMPORTANT:** Before downloading, you must:\n",
    "1. Go to https://huggingface.co/meta-llama/Llama-2-7b\n",
    "2. Accept Meta's Llama 2 license agreement\n",
    "3. Request access (usually instant approval)\n",
    "4. Login to HuggingFace CLI: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace token loaded from .env\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.1: Load HuggingFace token from .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv(Path(\"../.env\"))\n",
    "\n",
    "# Get HF token (try both common env var names)\n",
    "hf_token = os.environ.get(\"HF_API_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"HuggingFace token loaded from .env\")\n",
    "else:\n",
    "    print(\"No HF token in .env. Please run: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at ../models/selfrag_llama2_7b\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.2: Download model\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID = \"selfrag/selfrag_llama2_7b\"\n",
    "LOCAL_DIR = Path(\"../models/selfrag_llama2_7b\")\n",
    "\n",
    "# Download (requires ~14GB disk space)\n",
    "if not LOCAL_DIR.exists():\n",
    "    print(f\"Downloading {MODEL_ID}...\")\n",
    "    print(\"This may take 10-30 minutes depending on connection speed.\")\n",
    "    snapshot_download(\n",
    "        repo_id=MODEL_ID,\n",
    "        local_dir=str(LOCAL_DIR),\n",
    "        local_dir_use_symlinks=False,\n",
    "        token=hf_token,\n",
    "    )\n",
    "    print(f\"Downloaded to {LOCAL_DIR}\")\n",
    "else:\n",
    "    print(f\"Model already exists at {LOCAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files (12 files):\n",
      "  .cache (0.0 MB)\n",
      "  .gitattributes (0.0 MB)\n",
      "  README.md (0.0 MB)\n",
      "  added_tokens.json (0.0 MB)\n",
      "  config.json (0.0 MB)\n",
      "  generation_config.json (0.0 MB)\n",
      "  pytorch_model-00001-of-00002.bin (9976.8 MB)\n",
      "  pytorch_model-00002-of-00002.bin (3500.4 MB)\n",
      "  pytorch_model.bin.index.json (0.0 MB)\n",
      "  special_tokens_map.json (0.0 MB)\n",
      "  tokenizer.model (0.5 MB)\n",
      "  tokenizer_config.json (0.0 MB)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.3: Verify download\n",
    "if LOCAL_DIR.exists():\n",
    "    model_files = list(LOCAL_DIR.iterdir())\n",
    "    print(f\"Model files ({len(model_files)} files):\")\n",
    "    for f in sorted(model_files):\n",
    "        size_mb = f.stat().st_size / 1e6 if f.is_file() else 0\n",
    "        print(f\"  {f.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"Model directory does not exist. Please download first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Convert to GGUF Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clone llama.cpp and Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3.1: Clone llama.cpp\n",
    "# LLAMA_CPP_DIR = Path(\"../tools/llama.cpp\")\n",
    "\n",
    "# if not LLAMA_CPP_DIR.exists():\n",
    "#     print(\"Cloning llama.cpp...\")\n",
    "#     !git clone https://github.com/ggerganov/llama.cpp {LLAMA_CPP_DIR}\n",
    "#     print(f\"Cloned to {LLAMA_CPP_DIR}\")\n",
    "# else:\n",
    "#     print(f\"llama.cpp already exists at {LLAMA_CPP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3.2: Build llama.cpp with Metal support (Mac)\n",
    "# import subprocess\n",
    "\n",
    "# BUILD_DIR = LLAMA_CPP_DIR / \"build\"\n",
    "\n",
    "# print(\"Building llama.cpp with CMake and Metal support...\")\n",
    "\n",
    "# # Create build directory\n",
    "# BUILD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# # Configure with CMake (enable Metal)\n",
    "# result = subprocess.run(\n",
    "#     [\"cmake\", \"..\", \"-DLLAMA_METAL=ON\"],\n",
    "#     cwd=str(BUILD_DIR),\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "\n",
    "# if result.returncode != 0:\n",
    "#     print(f\"CMake configure failed: {result.stderr}\")\n",
    "# else:\n",
    "#     # Build\n",
    "#     result = subprocess.run(\n",
    "#         [\"cmake\", \"--build\", \".\", \"--config\", \"Release\", \"-j\"],\n",
    "#         cwd=str(BUILD_DIR),\n",
    "#         capture_output=True,\n",
    "#         text=True\n",
    "#     )\n",
    "\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"Build successful!\")\n",
    "#     else:\n",
    "#         print(f\"Build failed: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Install Python conversion dependencies\n",
    "# !uv pip install -q -r {LLAMA_CPP_DIR}/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convert HuggingFace to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 GGUF already exists: ../models/selfrag_llama2_7b.fp16.gguf\n",
      "FP16 GGUF size: 13.48 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.4: Convert to GGUF FP16\n",
    "FP16_GGUF = Path(\"../models/selfrag_llama2_7b.fp16.gguf\")\n",
    "\n",
    "if not FP16_GGUF.exists():\n",
    "    print(\"Converting to FP16 GGUF (this takes ~5-10 minutes)...\")\n",
    "    !python {LLAMA_CPP_DIR}/convert_hf_to_gguf.py {LOCAL_DIR} \\\n",
    "        --outfile {FP16_GGUF} \\\n",
    "        --outtype f16\n",
    "    print(f\"Converted to FP16 GGUF: {FP16_GGUF}\")\n",
    "else:\n",
    "    print(f\"FP16 GGUF already exists: {FP16_GGUF}\")\n",
    "\n",
    "# Check file size\n",
    "if FP16_GGUF.exists():\n",
    "    fp16_size = FP16_GGUF.stat().st_size / 1e9\n",
    "    print(f\"FP16 GGUF size: {fp16_size:.2f} GB\")  # Expected: ~13-14 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Quantize to Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4_K_M GGUF already exists: ../models/selfrag_llama2_7b.Q4_K_M.gguf\n",
      "Q4_K_M GGUF size: 4.08 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: Quantize to Q4_K_M (best quality/size balance)\n",
    "Q4_GGUF = Path(\"../models/selfrag_llama2_7b.Q4_K_M.gguf\")\n",
    "QUANTIZE_BIN = LLAMA_CPP_DIR / \"build\" / \"bin\" / \"llama-quantize\"\n",
    "\n",
    "if not Q4_GGUF.exists():\n",
    "    print(\"Quantizing to Q4_K_M (this takes ~2-5 minutes)...\")\n",
    "    !{QUANTIZE_BIN} {FP16_GGUF} {Q4_GGUF} Q4_K_M\n",
    "    print(f\"Quantized to Q4_K_M: {Q4_GGUF}\")\n",
    "else:\n",
    "    print(f\"Q4_K_M GGUF already exists: {Q4_GGUF}\")\n",
    "\n",
    "# Check file size\n",
    "if Q4_GGUF.exists():\n",
    "    q4_size = Q4_GGUF.stat().st_size / 1e9\n",
    "    print(f\"Q4_K_M GGUF size: {q4_size:.2f} GB\")  # Expected: ~3.8-4.2 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Validate Special Tokens (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ../models/selfrag_llama2_7b.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.1: Load GGUF model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Use Q4 if available, otherwise fall back to FP16\n",
    "MODEL_PATH = Q4_GGUF if Q4_GGUF.exists() else FP16_GGUF\n",
    "\n",
    "print(f\"Loading model: {MODEL_PATH}\")\n",
    "llm = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=2048,          # Context window (2048 safer for 16GB Mac)\n",
    "    n_batch=512,         # Batch size for prompt processing\n",
    "    n_gpu_layers=-1,     # Use all GPU layers (Metal)\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reflection tokens (from model's added_tokens.json):\n",
      "  retrieve: ['[Retrieval]', '[No Retrieval]', '[Continue to Use Evidence]']\n",
      "  isrel: ['[Relevant]', '[Irrelevant]']\n",
      "  issup: ['[Fully supported]', '[Partially supported]', '[No support / Contradictory]']\n",
      "  isuse: ['[Utility:5]', '[Utility:4]', '[Utility:3]', '[Utility:2]', '[Utility:1]']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.2: Define expected tokens (matching actual model tokens from added_tokens.json)\n",
    "EXPECTED_TOKENS = {\n",
    "    'retrieve': ['[Retrieval]', '[No Retrieval]', '[Continue to Use Evidence]'],\n",
    "    'isrel': ['[Relevant]', '[Irrelevant]'],\n",
    "    'issup': ['[Fully supported]', '[Partially supported]', '[No support / Contradictory]'],\n",
    "    'isuse': ['[Utility:5]', '[Utility:4]', '[Utility:3]', '[Utility:2]', '[Utility:1]'],\n",
    "}\n",
    "\n",
    "print(\"Expected reflection tokens (from model's added_tokens.json):\")\n",
    "for token_type, tokens in EXPECTED_TOKENS.items():\n",
    "    print(f\"  {token_type}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation WITHOUT passage...\n",
      "Generated: Paris is the capital of France.[Utility:5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.3: Test token generation (without passage)\n",
    "test_prompt = \"\"\"### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing generation WITHOUT passage...\")\n",
    "output = llm(test_prompt, max_tokens=100, stop=[\"###\"], echo=False)\n",
    "generated = output['choices'][0]['text']\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token validation:\n",
      "  retrieve: NOT FOUND - []\n",
      "  isrel: NOT FOUND - []\n",
      "  issup: NOT FOUND - []\n",
      "  isuse: FOUND - ['[Utility:5]']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.4: Check if model produces reflection tokens\n",
    "def check_token_presence(text, token_list):\n",
    "    \"\"\"Check which tokens from list appear in text.\"\"\"\n",
    "    found = [t for t in token_list if t in text]\n",
    "    return found\n",
    "\n",
    "print(\"\\nToken validation:\")\n",
    "for token_type, tokens in EXPECTED_TOKENS.items():\n",
    "    found = check_token_presence(generated, tokens)\n",
    "    status = \"FOUND\" if found else \"NOT FOUND\"\n",
    "    print(f\"  {token_type}: {status} - {found if found else '[]'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation WITH passage...\n",
      "Generated: [Relevant]Paris is the capital of France.[Fully supported][Utility:5]\n",
      "\n",
      "Token validation (with passage):\n",
      "  retrieve: NOT FOUND - []\n",
      "  isrel: FOUND - ['[Relevant]']\n",
      "  issup: FOUND - ['[Fully supported]']\n",
      "  isuse: FOUND - ['[Utility:5]']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4.5: Test with passage (Self-RAG format)\n",
    "test_with_passage = \"\"\"### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\n",
    "[Retrieval]<paragraph>Paris is the capital and largest city of France. It is located on the Seine River in the north of France.</paragraph>\"\"\"\n",
    "\n",
    "print(\"Testing generation WITH passage...\")\n",
    "output2 = llm(test_with_passage, max_tokens=100, stop=[\"###\"], echo=False)\n",
    "generated2 = output2['choices'][0]['text']\n",
    "print(f\"Generated: {generated2}\")\n",
    "\n",
    "print(\"\\nToken validation (with passage):\")\n",
    "for token_type, tokens in EXPECTED_TOKENS.items():\n",
    "    found = check_token_presence(generated2, tokens)\n",
    "    status = \"FOUND\" if found else \"NOT FOUND\"\n",
    "    print(f\"  {token_type}: {status} - {found if found else '[]'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Inference Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Self-RAG model: ../models/selfrag_llama2_7b.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded (persistent, single instance for all operations)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Use the SelfRAGGGUFInference class\n",
    "# Close the previous model to free memory\n",
    "del llm\n",
    "\n",
    "# Use our inference class\n",
    "inference = SelfRAGGGUFInference(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=2048,  # 2048 safer for 16GB Mac\n",
    "    n_gpu_layers=-1,  # Metal acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WITHOUT PASSAGE:\n",
      "============================================================\n",
      "Answer: The elements of negligence are:\n",
      "\n",
      "1.Duty of care:<paragraph><paragraph><paragraph><paragraph>\n",
      "2.Breach of duty: A breach of duty occurs when a person fails to uphold their duty of care.3. Causation: The third element of negligence is causation.4. Damages: The fourth element of negligence is damages.T...\n",
      "Retrieve: [No Retrieval]\n",
      "IsRel: None\n",
      "IsSup: None\n",
      "IsUse: [Utility:5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Test without passage\n",
    "result1 = inference.generate(\"What are the elements of negligence?\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITHOUT PASSAGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Answer: {result1.answer[:300]}...\" if len(result1.answer) > 300 else f\"Answer: {result1.answer}\")\n",
    "print(f\"Retrieve: {result1.retrieve}\")\n",
    "print(f\"IsRel: {result1.isrel}\")\n",
    "print(f\"IsSup: {result1.issup}\")\n",
    "print(f\"IsUse: {result1.isuse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WITH PASSAGE:\n",
      "============================================================\n",
      "Answer: The elements of negligence are: duty, breach, causation, and damages.\n",
      "Retrieve: [Retrieval]\n",
      "IsRel: [Relevant]\n",
      "IsSup: [Fully supported]\n",
      "IsUse: [Utility:5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.3: Test with passage\n",
    "test_passage = \"\"\"To establish negligence, the plaintiff must prove four elements: \n",
    "(1) duty of care - the defendant owed a legal duty to the plaintiff, \n",
    "(2) breach - the defendant breached that duty, \n",
    "(3) causation - the breach caused the plaintiff's injury, and \n",
    "(4) damages - the plaintiff suffered actual harm.\"\"\"\n",
    "\n",
    "result2 = inference.generate(\n",
    "    \"What are the elements of negligence?\",\n",
    "    passage=test_passage\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITH PASSAGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Answer: {result2.answer[:300]}...\" if len(result2.answer) > 300 else f\"Answer: {result2.answer}\")\n",
    "print(f\"Retrieve: {result2.retrieve}\")\n",
    "print(f\"IsRel: {result2.isrel}\")\n",
    "print(f\"IsSup: {result2.issup}\")\n",
    "print(f\"IsUse: {result2.isuse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Integrate with Existing Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on cpu\n",
      "Embedding dimension: 768\n",
      "Using CPU index\n",
      "Created IndexFlatIP index with dimension 768\n",
      "Index loaded from ../data/legalbench_embeddings/faiss_index.faiss\n",
      "Total documents in index: 326783\n",
      "Documents loaded from ../data/legalbench_embeddings/documents.pkl\n",
      "Loaded retriever with 326783 documents\n"
     ]
    }
   ],
   "source": [
    "# Cell 6.1: Load existing FAISS retriever\n",
    "INDEX_DIR = Path(\"../data/legalbench_embeddings\")\n",
    "\n",
    "# Check if index exists\n",
    "if INDEX_DIR.exists():\n",
    "    # Initialize embedding model\n",
    "    embedding_model = EmbeddingModel(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        device=\"cpu\"  # Use CPU for embedding on Mac\n",
    "    )\n",
    "    \n",
    "    # Create retriever and load index\n",
    "    retriever = LegalRetriever(\n",
    "        embedding_model=embedding_model,\n",
    "        top_k=3\n",
    "    )\n",
    "    retriever.load_index(str(INDEX_DIR))\n",
    "    \n",
    "    print(f\"Loaded retriever with {retriever.get_num_documents()} documents\")\n",
    "else:\n",
    "    print(f\"Index not found at {INDEX_DIR}\")\n",
    "    print(\"Please run the retrieval indexing notebook first.\")\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Does the NDA require the receiving party to return confidential information?\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 passages\n",
      "Top passage score: 0.6992\n",
      "\n",
      "--- Generated Output ---\n",
      "Answer: No, the NDA does not require the receiving party to return confidential information.\n",
      "\n",
      "Reflection Tokens:\n",
      "  Retrieve: [Retrieval]\n",
      "  IsRel: [Relevant]\n",
      "  IsSup: [No support / Contradictory]\n",
      "  IsUse: [Utility:5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6.2: Test full pipeline with retrieval\n",
    "if retriever is not None:\n",
    "    question = \"Does the NDA require the receiving party to return confidential information?\"\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = inference.generate_with_retrieval(question, retriever)\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(result['passages'])} passages\")\n",
    "    if result.get('passage_score'):\n",
    "        print(f\"Top passage score: {result['passage_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n--- Generated Output ---\")\n",
    "    output = result['output']\n",
    "    print(f\"Answer: {output.answer[:400]}...\" if len(output.answer) > 400 else f\"Answer: {output.answer}\")\n",
    "    print(f\"\\nReflection Tokens:\")\n",
    "    print(f\"  Retrieve: {output.retrieve}\")\n",
    "    print(f\"  IsRel: {output.isrel}\")\n",
    "    print(f\"  IsSup: {output.issup}\")\n",
    "    print(f\"  IsUse: {output.isuse}\")\n",
    "else:\n",
    "    print(\"Retriever not available. Skipping retrieval test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multiple questions...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question 1: What is the definition of confidential information in the NDA?\n",
      "============================================================\n",
      "Answer: The definition of confidential information in the NDA is typically outlined in Section 2 of the agreement.This section defines what information is considered confidential and is protected under the te...\n",
      "Tokens: IsRel=[Relevant], IsSup=[No support / Contradictory], IsUse=[Utility:5]\n",
      "\n",
      "============================================================\n",
      "Question 2: Can the receiving party disclose information to its employees?\n",
      "============================================================\n",
      "Answer: Yes, the receiving party can disclose information to its employees, but only to the extent that it is necessary for the receiving party's internal use and only after informing the employees of the res...\n",
      "Tokens: IsRel=[Relevant], IsSup=[Partially supported], IsUse=[Utility:5]\n",
      "\n",
      "============================================================\n",
      "Question 3: What happens if there is a breach of the NDA?\n",
      "============================================================\n",
      "Answer: If there is a breach of an NDA, the person who breached the agreement may be subject to legal action, including fines and other penalties....\n",
      "Tokens: IsRel=[Relevant], IsSup=[No support / Contradictory], IsUse=[Utility:5]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6.3: Test multiple questions\n",
    "if retriever is not None:\n",
    "    test_questions = [\n",
    "        \"What is the definition of confidential information in the NDA?\",\n",
    "        \"Can the receiving party disclose information to its employees?\",\n",
    "        \"What happens if there is a breach of the NDA?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing multiple questions...\\n\")\n",
    "    \n",
    "    for i, q in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {i}: {q}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = inference.generate_with_retrieval(q, retriever)\n",
    "        output = result['output']\n",
    "        \n",
    "        print(f\"Answer: {output.answer[:200]}...\")\n",
    "        print(f\"Tokens: IsRel={output.isrel}, IsSup={output.issup}, IsUse={output.isuse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Evaluation on LegalBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6889 queries\n",
      "\n",
      "Sample query: {'query': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?', 'snippets': [{'file_path': 'contractnli/CopAcc_NDA-and-ToP-Mentors_2.0_2017.txt', 'span': [11461, 11963], 'answer': 'Any and all proprietary rights, including but not limited to rights to and in inventions, patent rights, utility models, copyrights, trademarks and trade secrets, in and to any Confidential Information shall be and remain with the Participants respectively, and Mentor shall not have any right, license, title or interest in or to any Confidential Information, except the limited right to review, assess and help develop such Confidential Information in connection with the Copernicus Accelerator 2017.'}], 'dataset_source': 'ContractNLI'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.1: Load LegalBench queries\n",
    "QUERIES_FILE = Path(\"../data/legalbench-rag/queries.json\")\n",
    "\n",
    "if QUERIES_FILE.exists():\n",
    "    with open(QUERIES_FILE, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "    \n",
    "    queries = queries_data.get('tests', queries_data.get('queries', []))\n",
    "    print(f\"Loaded {len(queries)} queries\")\n",
    "    \n",
    "    # Show sample\n",
    "    if queries:\n",
    "        print(f\"\\nSample query: {queries[0]}\")\n",
    "else:\n",
    "    print(f\"Queries file not found: {QUERIES_FILE}\")\n",
    "    queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 776 ground truth labels\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.2: Load ground truth labels (if available)\n",
    "LABELS_FILE = Path(\"../data/training/legalbench_training_labels.json\")\n",
    "\n",
    "if LABELS_FILE.exists():\n",
    "    with open(LABELS_FILE, 'r') as f:\n",
    "        training_labels = json.load(f)\n",
    "    \n",
    "    # Create lookup by question\n",
    "    gt_labels = {ex.get('question', ex.get('query', '')): ex for ex in training_labels}\n",
    "    print(f\"Loaded {len(gt_labels)} ground truth labels\")\n",
    "else:\n",
    "    print(f\"Labels file not found: {LABELS_FILE}\")\n",
    "    gt_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 10 queries...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b52b4b6ba1f49258a9b1885dc3c1df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete. Processed 10 queries.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.3: Evaluate on queries\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Limit for testing (set to len(queries) for full evaluation)\n",
    "EVAL_LIMIT = 10\n",
    "\n",
    "if queries and retriever is not None:\n",
    "    results = []\n",
    "    token_accuracy = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    eval_queries = queries[:EVAL_LIMIT]\n",
    "    print(f\"Evaluating on {len(eval_queries)} queries...\\n\")\n",
    "    \n",
    "    for query_data in tqdm(eval_queries, desc=\"Evaluating\"):\n",
    "        # Handle different query formats\n",
    "        if isinstance(query_data, dict):\n",
    "            question = query_data.get('query', query_data.get('question', ''))\n",
    "        else:\n",
    "            question = str(query_data)\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt = gt_labels.get(question, {})\n",
    "        gt_tokens = gt.get('reflection_tokens', {})\n",
    "        \n",
    "        # Generate with retrieval\n",
    "        try:\n",
    "            output = inference.generate_with_retrieval(question, retriever)\n",
    "            pred = output['output']\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'predicted': {\n",
    "                    'answer': pred.answer,\n",
    "                    'retrieve': pred.retrieve,\n",
    "                    'isrel': pred.isrel,\n",
    "                    'issup': pred.issup,\n",
    "                    'isuse': pred.isuse,\n",
    "                },\n",
    "                'ground_truth': gt_tokens,\n",
    "                'passage_score': output.get('passage_score'),\n",
    "            })\n",
    "            \n",
    "            # Calculate token accuracy\n",
    "            for token_type in ['retrieve', 'isrel', 'issup', 'isuse']:\n",
    "                gt_val = gt_tokens.get(token_type)\n",
    "                pred_val = getattr(pred, token_type)\n",
    "                \n",
    "                if gt_val and pred_val:\n",
    "                    token_accuracy[token_type]['total'] += 1\n",
    "                    if gt_val == pred_val:\n",
    "                        token_accuracy[token_type]['correct'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error on question: {question[:50]}... - {e}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. Processed {len(results)} queries.\")\n",
    "else:\n",
    "    print(\"Cannot run evaluation: queries or retriever not available.\")\n",
    "    results = []\n",
    "    token_accuracy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS (Pre-trained Self-RAG 7B GGUF)\n",
      "================================================================================\n",
      "\n",
      "Model: selfrag/selfrag_llama2_7b (Q4_K_M GGUF)\n",
      "Queries evaluated: 10\n",
      "\n",
      "Reflection Token Accuracy:\n",
      "  RETRIEVE: 0.0% (0/10)\n",
      "  ISREL: 40.0% (4/10)\n",
      "  ISSUP: 0.0% (0/10)\n",
      "  ISUSE: 50.0% (5/10)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.4: Print evaluation results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS (Pre-trained Self-RAG 7B GGUF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModel: selfrag/selfrag_llama2_7b (Q4_K_M GGUF)\")\n",
    "print(f\"Queries evaluated: {len(results)}\")\n",
    "\n",
    "if token_accuracy:\n",
    "    print(\"\\nReflection Token Accuracy:\")\n",
    "    for token_type, counts in token_accuracy.items():\n",
    "        if counts['total'] > 0:\n",
    "            acc = counts['correct'] / counts['total'] * 100\n",
    "            print(f\"  {token_type.upper()}: {acc:.1f}% ({counts['correct']}/{counts['total']})\")\n",
    "        else:\n",
    "            print(f\"  {token_type.upper()}: N/A (no ground truth)\")\n",
    "else:\n",
    "    print(\"\\nNo token accuracy data available (no ground truth labels).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Token Distribution Analysis:\n",
      "----------------------------------------\n",
      "\n",
      "RETRIEVE:\n",
      "  [Retrieval]: 10 (100.0%)\n",
      "\n",
      "ISREL:\n",
      "  [Relevant]: 10 (100.0%)\n",
      "\n",
      "ISSUP:\n",
      "  [Fully supported]: 9 (90.0%)\n",
      "  [Partially supported]: 1 (10.0%)\n",
      "\n",
      "ISUSE:\n",
      "  [Utility:5]: 10 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.5: Analyze token distribution\n",
    "if results:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Token Distribution Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    for token_type in ['retrieve', 'isrel', 'issup', 'isuse']:\n",
    "        values = [r['predicted'][token_type] for r in results if r['predicted'][token_type]]\n",
    "        counter = Counter(values)\n",
    "        print(f\"\\n{token_type.upper()}:\")\n",
    "        for val, count in counter.most_common():\n",
    "            pct = count / len(values) * 100 if values else 0\n",
    "            print(f\"  {val}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to ../results/selfrag_7b_gguf_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 7.6: Save evaluation results\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RESULTS_FILE = RESULTS_DIR / \"selfrag_7b_gguf_evaluation.json\"\n",
    "\n",
    "evaluation_summary = {\n",
    "    'model': 'selfrag/selfrag_llama2_7b (Q4_K_M GGUF)',\n",
    "    'num_queries': len(results),\n",
    "    'token_accuracy': dict(token_accuracy),\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "with open(RESULTS_FILE, 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Model Download**: Downloaded pre-trained Self-RAG 7B from HuggingFace\n",
    "2. **GGUF Conversion**: Converted to Q4_K_M quantization (~4GB)\n",
    "3. **Token Validation**: Verified reflection tokens work after conversion\n",
    "4. **Inference Integration**: Used `SelfRAGGGUFInference` class with llama.cpp\n",
    "5. **Retriever Integration**: Combined with existing `LegalRetriever` for RAG\n",
    "6. **Evaluation**: Tested on LegalBench queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-rag-legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
