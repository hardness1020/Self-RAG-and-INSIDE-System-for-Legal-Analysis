{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-RAG Interactive Demo\n",
    "\n",
    "Interactive demonstration of the complete Self-RAG system for legal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.self_rag.inference import load_pipeline_from_config\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Complete Self-RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "print(\"Loading Self-RAG pipeline...\")\n",
    "\n",
    "pipeline = load_pipeline_from_config(\n",
    "    retrieval_config_path='../configs/retrieval_config.yaml',\n",
    "    generator_config_path='../configs/generator_config.yaml',\n",
    "    retriever_index_dir='../data/embeddings',\n",
    "    generator_weights_path='../models/generator_lora/final',\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test with Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_question(question):\n",
    "    \"\"\"Demonstrate Self-RAG with a question.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get answer\n",
    "    result = pipeline.answer_question(question)\n",
    "    \n",
    "    # Display formatted response\n",
    "    formatted = pipeline.format_response(\n",
    "        result,\n",
    "        include_passages=True,\n",
    "        include_reflection=True\n",
    "    )\n",
    "    \n",
    "    print(formatted)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic negligence question\n",
    "result1 = demo_question(\"What are the four elements of negligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Specific doctrine\n",
    "result2 = demo_question(\"What is res ipsa loquitur and when does it apply?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Defenses\n",
    "result3 = demo_question(\"What is the difference between comparative negligence and assumption of risk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Question & Answer\n",
    "\n",
    "Enter your own questions below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode\n",
    "your_question = input(\"Enter your legal question: \")\n",
    "\n",
    "if your_question:\n",
    "    result = demo_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Reflection Tokens\n",
    "\n",
    "Examine the self-verification in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reflection(result):\n",
    "    \"\"\"Analyze reflection tokens from a result.\"\"\"\n",
    "    reflection = result['reflection']\n",
    "    \n",
    "    print(\"\\nReflection Token Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nüìç Retrieve: {reflection.get('retrieve', 'N/A')}\")\n",
    "    print(\"   ‚Üí Did the model decide to retrieve evidence?\")\n",
    "    \n",
    "    print(f\"\\nüîç ISREL (Relevance): {reflection.get('isrel', 'N/A')}\")\n",
    "    print(\"   ‚Üí Is the retrieved passage relevant?\")\n",
    "    \n",
    "    print(f\"\\n‚úì ISSUP (Support): {reflection.get('issup', 'N/A')}\")\n",
    "    print(\"   ‚Üí Is the answer supported by evidence?\")\n",
    "    print(\"   ‚Üí Hallucination detection!\")\n",
    "    \n",
    "    print(f\"\\n‚≠ê ISUSE (Utility): {reflection.get('isuse', 'N/A')}\")\n",
    "    print(\"   ‚Üí Overall response quality (1-5)\")\n",
    "    \n",
    "    print(f\"\\nüìä Overall Score: {result['score']:.2f}\")\n",
    "    \n",
    "    # Hallucination check\n",
    "    support = reflection.get('issup', '')\n",
    "    if 'No Support' in support:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Potential hallucination detected!\")\n",
    "    elif 'Fully Supported' in support:\n",
    "        print(\"\\n‚úÖ Response is fully supported by evidence\")\n",
    "\n",
    "# Analyze previous results\n",
    "analyze_reflection(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing\n",
    "\n",
    "Process multiple questions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch questions\n",
    "questions = [\n",
    "    \"What is causation in negligence?\",\n",
    "    \"What damages can be recovered?\",\n",
    "    \"What is professional malpractice?\",\n",
    "]\n",
    "\n",
    "# Process all\n",
    "results = pipeline.answer_batch(questions)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBatch Processing Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (q, r) in enumerate(zip(questions, results), 1):\n",
    "    print(f\"\\n{i}. {q}\")\n",
    "    print(f\"   Score: {r['score']:.2f}\")\n",
    "    print(f\"   Support: {r['reflection'].get('issup', 'N/A')}\")\n",
    "    print(f\"   Answer: {r['answer'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for analysis\n",
    "output = {\n",
    "    'questions': questions,\n",
    "    'results': [\n",
    "        {\n",
    "            'question': r['question'],\n",
    "            'answer': r['answer'],\n",
    "            'reflection': r['reflection'],\n",
    "            'score': r['score']\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../results/demo_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results saved to ../results/demo_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Demo complete!\n",
    "- ‚úÖ Tested Self-RAG on legal questions\n",
    "- ‚úÖ Analyzed reflection tokens\n",
    "- ‚úÖ Demonstrated hallucination detection\n",
    "- ‚úÖ Processed batch questions\n",
    "- ‚úÖ Exported results\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Adaptive Retrieval**: Model decides when to retrieve evidence\n",
    "2. **Self-Verification**: Reflection tokens provide quality assessment\n",
    "3. **Hallucination Detection**: ISSUP token identifies unsupported claims\n",
    "4. **Transparency**: See exactly why the model made each decision\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Evaluate on your own legal questions\n",
    "- Compare with baseline models\n",
    "- Analyze patterns in reflection tokens\n",
    "- Use for your DSC261 project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
